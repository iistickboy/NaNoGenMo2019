{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Writing Prompt Responder",
      "provenance": [],
      "collapsed_sections": [
        "BT__brhBCvJu",
        "LdpZQXknFNY3"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H7LoMj4GA4n_",
        "colab_type": "text"
      },
      "source": [
        "#  Finetune GPT-2 on Reddit Data\n",
        "\n",
        "by [Max Woolf](http://minimaxir.com)\n",
        "\n",
        "A variant of the [default notebook](https://colab.research.google.com/drive/1VLG8e7YSEwypxU-noRNhsv5dW4NfTGce) optimized for short-form titles. It is recommended to be familiar with that notebook before using this one.\n",
        "\n",
        "This example uses a CSV export of Reddit data via BigQuery (see this post for more information).\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KBkpRgBCBS2_",
        "colab_type": "code",
        "outputId": "edd4f3f2-bd06-4f4f-aeea-e80df42d88be",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 233
        }
      },
      "source": [
        "!pip install -q gpt-2-simple\n",
        "import gpt_2_simple as gpt2\n",
        "from datetime import datetime\n",
        "from google.colab import files"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[?25l\r\u001b[K     |▌                               | 10kB 23.8MB/s eta 0:00:01\r\u001b[K     |█                               | 20kB 27.6MB/s eta 0:00:01\r\u001b[K     |█▌                              | 30kB 33.2MB/s eta 0:00:01\r\u001b[K     |██                              | 40kB 4.4MB/s eta 0:00:01\r\u001b[K     |██▌                             | 51kB 5.4MB/s eta 0:00:01\r\u001b[K     |███                             | 61kB 6.3MB/s eta 0:00:01\r\u001b[K     |███▋                            | 71kB 7.2MB/s eta 0:00:01\r\u001b[K     |████                            | 81kB 7.1MB/s eta 0:00:01\r\u001b[K     |████▋                           | 92kB 7.9MB/s eta 0:00:01\r\u001b[K     |█████                           | 102kB 8.6MB/s eta 0:00:01\r\u001b[K     |█████▋                          | 112kB 8.6MB/s eta 0:00:01\r\u001b[K     |██████                          | 122kB 8.6MB/s eta 0:00:01\r\u001b[K     |██████▋                         | 133kB 8.6MB/s eta 0:00:01\r\u001b[K     |███████▏                        | 143kB 8.6MB/s eta 0:00:01\r\u001b[K     |███████▋                        | 153kB 8.6MB/s eta 0:00:01\r\u001b[K     |████████▏                       | 163kB 8.6MB/s eta 0:00:01\r\u001b[K     |████████▋                       | 174kB 8.6MB/s eta 0:00:01\r\u001b[K     |█████████▏                      | 184kB 8.6MB/s eta 0:00:01\r\u001b[K     |█████████▊                      | 194kB 8.6MB/s eta 0:00:01\r\u001b[K     |██████████▏                     | 204kB 8.6MB/s eta 0:00:01\r\u001b[K     |██████████▊                     | 215kB 8.6MB/s eta 0:00:01\r\u001b[K     |███████████▏                    | 225kB 8.6MB/s eta 0:00:01\r\u001b[K     |███████████▊                    | 235kB 8.6MB/s eta 0:00:01\r\u001b[K     |████████████▏                   | 245kB 8.6MB/s eta 0:00:01\r\u001b[K     |████████████▊                   | 256kB 8.6MB/s eta 0:00:01\r\u001b[K     |█████████████▎                  | 266kB 8.6MB/s eta 0:00:01\r\u001b[K     |█████████████▊                  | 276kB 8.6MB/s eta 0:00:01\r\u001b[K     |██████████████▎                 | 286kB 8.6MB/s eta 0:00:01\r\u001b[K     |██████████████▊                 | 296kB 8.6MB/s eta 0:00:01\r\u001b[K     |███████████████▎                | 307kB 8.6MB/s eta 0:00:01\r\u001b[K     |███████████████▉                | 317kB 8.6MB/s eta 0:00:01\r\u001b[K     |████████████████▎               | 327kB 8.6MB/s eta 0:00:01\r\u001b[K     |████████████████▉               | 337kB 8.6MB/s eta 0:00:01\r\u001b[K     |█████████████████▎              | 348kB 8.6MB/s eta 0:00:01\r\u001b[K     |█████████████████▉              | 358kB 8.6MB/s eta 0:00:01\r\u001b[K     |██████████████████▎             | 368kB 8.6MB/s eta 0:00:01\r\u001b[K     |██████████████████▉             | 378kB 8.6MB/s eta 0:00:01\r\u001b[K     |███████████████████▍            | 389kB 8.6MB/s eta 0:00:01\r\u001b[K     |███████████████████▉            | 399kB 8.6MB/s eta 0:00:01\r\u001b[K     |████████████████████▍           | 409kB 8.6MB/s eta 0:00:01\r\u001b[K     |████████████████████▉           | 419kB 8.6MB/s eta 0:00:01\r\u001b[K     |█████████████████████▍          | 430kB 8.6MB/s eta 0:00:01\r\u001b[K     |██████████████████████          | 440kB 8.6MB/s eta 0:00:01\r\u001b[K     |██████████████████████▍         | 450kB 8.6MB/s eta 0:00:01\r\u001b[K     |███████████████████████         | 460kB 8.6MB/s eta 0:00:01\r\u001b[K     |███████████████████████▍        | 471kB 8.6MB/s eta 0:00:01\r\u001b[K     |████████████████████████        | 481kB 8.6MB/s eta 0:00:01\r\u001b[K     |████████████████████████▍       | 491kB 8.6MB/s eta 0:00:01\r\u001b[K     |█████████████████████████       | 501kB 8.6MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▌      | 512kB 8.6MB/s eta 0:00:01\r\u001b[K     |██████████████████████████      | 522kB 8.6MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▌     | 532kB 8.6MB/s eta 0:00:01\r\u001b[K     |███████████████████████████     | 542kB 8.6MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▌    | 552kB 8.6MB/s eta 0:00:01\r\u001b[K     |████████████████████████████    | 563kB 8.6MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▌   | 573kB 8.6MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████   | 583kB 8.6MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▌  | 593kB 8.6MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 604kB 8.6MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▌ | 614kB 8.6MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████ | 624kB 8.6MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▋| 634kB 8.6MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 645kB 8.6MB/s \n",
            "\u001b[?25h  Building wheel for gpt-2-simple (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:\n",
            "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
            "For more information, please see:\n",
            "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
            "  * https://github.com/tensorflow/addons\n",
            "  * https://github.com/tensorflow/io (for I/O related ops)\n",
            "If you depend on functionality not listed there, please file an issue.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bj2IJLHP3KwE",
        "colab_type": "text"
      },
      "source": [
        "## GPU"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sUmTooTW3osf",
        "colab_type": "code",
        "outputId": "aef4aee5-e50a-4122-8362-9b607a0bf6b6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 306
        }
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Tue Nov 12 21:41:41 2019       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 430.50       Driver Version: 418.67       CUDA Version: 10.1     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   54C    P0    31W / 250W |      0MiB / 16280MiB |      0%      Default |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                       GPU Memory |\n",
            "|  GPU       PID   Type   Process name                             Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0wXB05bPDYxS",
        "colab_type": "text"
      },
      "source": [
        "## Downloading GPT-2\n",
        "\n",
        "The default query returns 1.3MB of data, so probably should only use `124M` GPT-2 to finetune. If working with more Reddity data, then migrate to `355M`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P8wSlgXoDPCR",
        "colab_type": "code",
        "outputId": "039f3448-98dd-4371-c638-861945681411",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        }
      },
      "source": [
        "gpt2.download_gpt2(model_name=\"355M\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Fetching checkpoint: 1.05Mit [00:00, 374Mit/s]                                                      \n",
            "Fetching encoder.json: 1.05Mit [00:00, 86.5Mit/s]                                                   \n",
            "Fetching hparams.json: 1.05Mit [00:00, 457Mit/s]                                                    \n",
            "Fetching model.ckpt.data-00000-of-00001: 1.42Git [00:07, 201Mit/s]                                  \n",
            "Fetching model.ckpt.index: 1.05Mit [00:00, 358Mit/s]                                                \n",
            "Fetching model.ckpt.meta: 1.05Mit [00:00, 85.2Mit/s]                                                \n",
            "Fetching vocab.bpe: 1.05Mit [00:00, 110Mit/s]                                                       \n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N8KXuKWzQSsN",
        "colab_type": "text"
      },
      "source": [
        "## Mounting Google Drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "puq4iC6vUAHc",
        "colab_type": "code",
        "outputId": "945d8225-51c3-40eb-85a9-af8ca584aac2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "gpt2.mount_gdrive()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BT__brhBCvJu",
        "colab_type": "text"
      },
      "source": [
        "## Uploading a Text File to be Trained to Colaboratory\n",
        "\n",
        "A single-column CSV is expected."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6OFnPCLADfll",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "file_name = \"AI Responses 10-23-2019.txt\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HeeSKtNWUedE",
        "colab_type": "text"
      },
      "source": [
        "If your text file is larger than 10MB, it is recommended to upload that file to Google Drive first, then copy that file from Google Drive to the Colaboratory VM."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Z6okFD8VKtS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "gpt2.copy_file_from_gdrive(file_name)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LdpZQXknFNY3",
        "colab_type": "text"
      },
      "source": [
        "## Finetune GPT-2\n",
        "\n",
        "Providing a single-column CSV will automatically add `<|startoftext|>` and `<|endoftext|>` tokens appropriately.\n",
        "\n",
        "Short form text is more likely to overfit, so train it with fewer steps than you would for longform content."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aeXshJM-Cuaf",
        "colab_type": "code",
        "outputId": "e5cd9592-bb23-4d1d-a443-29ad4c7f04f9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "sess = gpt2.start_tf_sess()\n",
        "\n",
        "gpt2.finetune(sess,\n",
        "              dataset=file_name,\n",
        "              model_name='355M',\n",
        "              steps=9000,\n",
        "              restore_from='latest',\n",
        "              run_name='run42K',\n",
        "              print_every=10,\n",
        "              sample_every=9000,\n",
        "              )"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/gpt_2_simple/src/sample.py:17: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/gpt_2_simple/src/memory_saving_gradients.py:62: get_backward_walk_ops (from tensorflow.contrib.graph_editor.select) is deprecated and will be removed after 2019-06-06.\n",
            "Instructions for updating:\n",
            "Please use tensorflow.python.ops.op_selector.get_backward_walk_ops.\n",
            "Loading checkpoint models/355M/model.ckpt\n",
            "INFO:tensorflow:Restoring parameters from models/355M/model.ckpt\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r  0%|          | 0/1 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Loading dataset...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r100%|██████████| 1/1 [00:11<00:00, 11.09s/it]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "dataset has 1947763 tokens\n",
            "Training...\n",
            "[10 | 18.15] loss=2.88 avg=2.88\n",
            "[20 | 26.85] loss=3.49 avg=3.19\n",
            "[30 | 35.59] loss=3.00 avg=3.12\n",
            "[40 | 44.30] loss=2.79 avg=3.04\n",
            "[50 | 53.00] loss=3.21 avg=3.08\n",
            "[60 | 61.71] loss=2.52 avg=2.98\n",
            "[70 | 70.37] loss=3.11 avg=3.00\n",
            "[80 | 79.08] loss=3.24 avg=3.03\n",
            "[90 | 87.75] loss=2.90 avg=3.02\n",
            "[100 | 96.46] loss=2.74 avg=2.99\n",
            "[110 | 105.14] loss=3.16 avg=3.00\n",
            "[120 | 113.84] loss=3.24 avg=3.03\n",
            "[130 | 122.54] loss=3.04 avg=3.03\n",
            "[140 | 131.22] loss=3.15 avg=3.04\n",
            "[150 | 139.90] loss=2.96 avg=3.03\n",
            "[160 | 148.59] loss=2.85 avg=3.02\n",
            "[170 | 157.28] loss=3.13 avg=3.02\n",
            "[180 | 165.96] loss=2.60 avg=3.00\n",
            "[190 | 174.68] loss=3.18 avg=3.01\n",
            "[200 | 183.36] loss=2.19 avg=2.96\n",
            "[210 | 192.05] loss=2.53 avg=2.94\n",
            "[220 | 200.74] loss=3.51 avg=2.97\n",
            "[230 | 209.43] loss=3.32 avg=2.99\n",
            "[240 | 218.15] loss=2.83 avg=2.98\n",
            "[250 | 226.86] loss=3.10 avg=2.98\n",
            "[260 | 235.56] loss=2.47 avg=2.96\n",
            "[270 | 244.25] loss=3.27 avg=2.98\n",
            "[280 | 252.92] loss=2.90 avg=2.97\n",
            "[290 | 261.60] loss=3.07 avg=2.98\n",
            "[300 | 270.31] loss=2.82 avg=2.97\n",
            "[310 | 279.00] loss=2.12 avg=2.94\n",
            "[320 | 287.69] loss=2.77 avg=2.93\n",
            "[330 | 296.38] loss=3.03 avg=2.94\n",
            "[340 | 305.07] loss=2.47 avg=2.92\n",
            "[350 | 313.75] loss=2.29 avg=2.90\n",
            "[360 | 322.44] loss=2.94 avg=2.90\n",
            "[370 | 331.13] loss=2.56 avg=2.89\n",
            "[380 | 339.85] loss=3.26 avg=2.90\n",
            "[390 | 348.57] loss=3.29 avg=2.91\n",
            "[400 | 357.27] loss=2.69 avg=2.91\n",
            "[410 | 365.94] loss=3.09 avg=2.91\n",
            "[420 | 374.62] loss=2.54 avg=2.90\n",
            "[430 | 383.31] loss=3.25 avg=2.91\n",
            "[440 | 392.01] loss=3.35 avg=2.92\n",
            "[450 | 400.68] loss=2.55 avg=2.91\n",
            "[460 | 409.36] loss=3.19 avg=2.92\n",
            "[470 | 418.08] loss=3.29 avg=2.93\n",
            "[480 | 426.77] loss=2.53 avg=2.92\n",
            "[490 | 435.46] loss=2.82 avg=2.92\n",
            "[500 | 444.14] loss=3.03 avg=2.92\n",
            "[510 | 452.83] loss=2.71 avg=2.91\n",
            "[520 | 461.53] loss=3.13 avg=2.92\n",
            "[530 | 470.21] loss=3.24 avg=2.93\n",
            "[540 | 478.88] loss=2.51 avg=2.92\n",
            "[550 | 487.55] loss=3.08 avg=2.92\n",
            "[560 | 496.24] loss=3.20 avg=2.93\n",
            "[570 | 504.94] loss=3.33 avg=2.94\n",
            "[580 | 513.63] loss=2.93 avg=2.94\n",
            "[590 | 522.32] loss=2.67 avg=2.93\n",
            "[600 | 531.03] loss=3.40 avg=2.94\n",
            "[610 | 539.73] loss=3.30 avg=2.95\n",
            "[620 | 548.41] loss=3.10 avg=2.95\n",
            "[630 | 557.10] loss=3.39 avg=2.96\n",
            "[640 | 565.76] loss=2.90 avg=2.96\n",
            "[650 | 574.46] loss=2.96 avg=2.96\n",
            "[660 | 583.16] loss=2.57 avg=2.95\n",
            "[670 | 591.87] loss=2.88 avg=2.95\n",
            "[680 | 600.57] loss=3.01 avg=2.95\n",
            "[690 | 609.27] loss=2.94 avg=2.95\n",
            "[700 | 617.97] loss=3.30 avg=2.96\n",
            "[710 | 626.66] loss=3.80 avg=2.97\n",
            "[720 | 635.35] loss=2.88 avg=2.97\n",
            "[730 | 644.05] loss=2.82 avg=2.97\n",
            "[740 | 652.77] loss=3.08 avg=2.97\n",
            "[750 | 661.49] loss=2.31 avg=2.96\n",
            "[760 | 670.18] loss=3.48 avg=2.97\n",
            "[770 | 678.89] loss=2.93 avg=2.97\n",
            "[780 | 687.56] loss=2.69 avg=2.96\n",
            "[790 | 696.25] loss=2.19 avg=2.95\n",
            "[800 | 704.96] loss=3.28 avg=2.96\n",
            "[810 | 713.65] loss=2.78 avg=2.95\n",
            "[820 | 722.35] loss=1.96 avg=2.93\n",
            "[830 | 731.04] loss=3.18 avg=2.94\n",
            "[840 | 739.73] loss=3.18 avg=2.94\n",
            "[850 | 748.43] loss=2.67 avg=2.94\n",
            "[860 | 757.10] loss=2.45 avg=2.93\n",
            "[870 | 765.80] loss=2.78 avg=2.93\n",
            "[880 | 774.51] loss=2.50 avg=2.92\n",
            "[890 | 783.22] loss=2.86 avg=2.92\n",
            "[900 | 791.91] loss=2.85 avg=2.92\n",
            "[910 | 800.60] loss=2.83 avg=2.92\n",
            "[920 | 809.30] loss=2.41 avg=2.91\n",
            "[930 | 817.99] loss=1.95 avg=2.89\n",
            "[940 | 826.65] loss=2.48 avg=2.89\n",
            "[950 | 835.39] loss=3.12 avg=2.89\n",
            "[960 | 844.10] loss=2.93 avg=2.89\n",
            "[970 | 852.78] loss=3.08 avg=2.89\n",
            "[980 | 861.45] loss=2.68 avg=2.89\n",
            "[990 | 870.14] loss=3.02 avg=2.89\n",
            "[1000 | 878.81] loss=2.18 avg=2.88\n",
            "Saving checkpoint/run42K/model-1000\n",
            "[1010 | 895.67] loss=2.68 avg=2.88\n",
            "[1020 | 904.35] loss=3.19 avg=2.88\n",
            "[1030 | 913.05] loss=2.05 avg=2.87\n",
            "[1040 | 921.73] loss=3.73 avg=2.88\n",
            "[1050 | 930.42] loss=3.03 avg=2.88\n",
            "[1060 | 939.10] loss=3.34 avg=2.89\n",
            "[1070 | 947.79] loss=2.59 avg=2.89\n",
            "[1080 | 956.47] loss=2.81 avg=2.89\n",
            "[1090 | 965.17] loss=2.95 avg=2.89\n",
            "[1100 | 973.88] loss=3.10 avg=2.89\n",
            "[1110 | 982.58] loss=2.59 avg=2.89\n",
            "[1120 | 991.26] loss=2.91 avg=2.89\n",
            "[1130 | 999.95] loss=3.19 avg=2.89\n",
            "[1140 | 1008.63] loss=3.47 avg=2.90\n",
            "[1150 | 1017.28] loss=3.07 avg=2.90\n",
            "[1160 | 1025.96] loss=1.91 avg=2.89\n",
            "[1170 | 1034.65] loss=3.40 avg=2.89\n",
            "[1180 | 1043.31] loss=3.22 avg=2.90\n",
            "[1190 | 1051.98] loss=2.73 avg=2.90\n",
            "[1200 | 1060.67] loss=1.83 avg=2.88\n",
            "[1210 | 1069.36] loss=2.61 avg=2.88\n",
            "[1220 | 1078.01] loss=2.89 avg=2.88\n",
            "[1230 | 1086.69] loss=2.27 avg=2.87\n",
            "[1240 | 1095.37] loss=2.18 avg=2.86\n",
            "[1250 | 1104.09] loss=2.70 avg=2.86\n",
            "[1260 | 1112.76] loss=2.07 avg=2.85\n",
            "[1270 | 1121.46] loss=2.32 avg=2.84\n",
            "[1280 | 1130.15] loss=2.75 avg=2.84\n",
            "[1290 | 1138.82] loss=3.26 avg=2.84\n",
            "[1300 | 1147.52] loss=2.63 avg=2.84\n",
            "[1310 | 1156.21] loss=2.59 avg=2.84\n",
            "[1320 | 1164.90] loss=3.13 avg=2.84\n",
            "[1330 | 1173.58] loss=2.55 avg=2.84\n",
            "[1340 | 1182.30] loss=2.86 avg=2.84\n",
            "[1350 | 1190.98] loss=2.92 avg=2.84\n",
            "[1360 | 1199.69] loss=2.02 avg=2.83\n",
            "[1370 | 1208.39] loss=2.63 avg=2.83\n",
            "[1380 | 1217.07] loss=3.36 avg=2.83\n",
            "[1390 | 1225.76] loss=2.40 avg=2.83\n",
            "[1400 | 1234.45] loss=2.81 avg=2.83\n",
            "[1410 | 1243.16] loss=3.29 avg=2.83\n",
            "[1420 | 1251.86] loss=1.65 avg=2.82\n",
            "[1430 | 1260.55] loss=2.57 avg=2.81\n",
            "[1440 | 1269.23] loss=2.92 avg=2.81\n",
            "[1450 | 1277.95] loss=3.21 avg=2.82\n",
            "[1460 | 1286.64] loss=2.06 avg=2.81\n",
            "[1470 | 1295.32] loss=2.08 avg=2.80\n",
            "[1480 | 1304.01] loss=1.73 avg=2.79\n",
            "[1490 | 1312.71] loss=2.91 avg=2.79\n",
            "[1500 | 1321.39] loss=2.72 avg=2.79\n",
            "[1510 | 1330.06] loss=2.33 avg=2.78\n",
            "[1520 | 1338.75] loss=3.03 avg=2.78\n",
            "[1530 | 1347.40] loss=3.40 avg=2.79\n",
            "[1540 | 1356.10] loss=2.26 avg=2.79\n",
            "[1550 | 1364.80] loss=0.98 avg=2.76\n",
            "[1560 | 1373.48] loss=3.26 avg=2.77\n",
            "[1570 | 1382.16] loss=2.74 avg=2.77\n",
            "[1580 | 1390.85] loss=2.39 avg=2.76\n",
            "[1590 | 1399.52] loss=2.86 avg=2.77\n",
            "[1600 | 1408.20] loss=2.54 avg=2.76\n",
            "[1610 | 1416.89] loss=2.32 avg=2.76\n",
            "[1620 | 1425.56] loss=2.42 avg=2.75\n",
            "[1630 | 1434.26] loss=3.49 avg=2.76\n",
            "[1640 | 1442.94] loss=3.18 avg=2.77\n",
            "[1650 | 1451.63] loss=3.94 avg=2.78\n",
            "[1660 | 1460.34] loss=2.34 avg=2.78\n",
            "[1670 | 1469.02] loss=3.17 avg=2.78\n",
            "[1680 | 1477.71] loss=2.29 avg=2.78\n",
            "[1690 | 1486.39] loss=2.47 avg=2.77\n",
            "[1700 | 1495.06] loss=3.10 avg=2.78\n",
            "[1710 | 1503.79] loss=2.41 avg=2.77\n",
            "[1720 | 1512.47] loss=2.52 avg=2.77\n",
            "[1730 | 1521.16] loss=1.90 avg=2.76\n",
            "[1740 | 1529.83] loss=2.29 avg=2.75\n",
            "[1750 | 1538.53] loss=2.32 avg=2.75\n",
            "[1760 | 1547.22] loss=3.35 avg=2.75\n",
            "[1770 | 1555.90] loss=2.74 avg=2.75\n",
            "[1780 | 1564.62] loss=2.65 avg=2.75\n",
            "[1790 | 1573.29] loss=2.90 avg=2.75\n",
            "[1800 | 1581.99] loss=2.49 avg=2.75\n",
            "[1810 | 1590.69] loss=2.67 avg=2.75\n",
            "[1820 | 1599.38] loss=1.53 avg=2.74\n",
            "[1830 | 1608.08] loss=2.39 avg=2.73\n",
            "[1840 | 1616.80] loss=2.85 avg=2.73\n",
            "[1850 | 1625.50] loss=2.45 avg=2.73\n",
            "[1860 | 1634.20] loss=2.00 avg=2.72\n",
            "[1870 | 1642.89] loss=3.14 avg=2.73\n",
            "[1880 | 1651.58] loss=2.20 avg=2.72\n",
            "[1890 | 1660.23] loss=3.18 avg=2.73\n",
            "[1900 | 1668.91] loss=2.17 avg=2.72\n",
            "[1910 | 1677.59] loss=2.32 avg=2.71\n",
            "[1920 | 1686.28] loss=3.16 avg=2.72\n",
            "[1930 | 1694.98] loss=1.77 avg=2.71\n",
            "[1940 | 1703.70] loss=1.34 avg=2.69\n",
            "[1950 | 1712.39] loss=2.74 avg=2.69\n",
            "[1960 | 1721.09] loss=3.44 avg=2.70\n",
            "[1970 | 1729.80] loss=1.89 avg=2.69\n",
            "[1980 | 1738.47] loss=1.17 avg=2.67\n",
            "[1990 | 1747.17] loss=2.66 avg=2.67\n",
            "[2000 | 1755.84] loss=2.45 avg=2.67\n",
            "Saving checkpoint/run42K/model-2000\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/training/saver.py:963: remove_checkpoint (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use standard file APIs to delete files with this prefix.\n",
            "[2010 | 1771.92] loss=2.39 avg=2.67\n",
            "[2020 | 1780.60] loss=3.48 avg=2.68\n",
            "[2030 | 1789.28] loss=2.23 avg=2.67\n",
            "[2040 | 1797.98] loss=1.82 avg=2.66\n",
            "[2050 | 1806.69] loss=3.09 avg=2.67\n",
            "[2060 | 1815.37] loss=1.85 avg=2.66\n",
            "[2070 | 1824.10] loss=1.38 avg=2.64\n",
            "[2080 | 1832.79] loss=2.56 avg=2.64\n",
            "[2090 | 1841.46] loss=2.72 avg=2.64\n",
            "[2100 | 1850.12] loss=2.77 avg=2.64\n",
            "[2110 | 1858.77] loss=2.83 avg=2.65\n",
            "[2120 | 1867.47] loss=2.18 avg=2.64\n",
            "[2130 | 1876.15] loss=2.24 avg=2.64\n",
            "[2140 | 1884.83] loss=2.56 avg=2.64\n",
            "[2150 | 1893.52] loss=1.31 avg=2.62\n",
            "[2160 | 1902.25] loss=2.64 avg=2.62\n",
            "[2170 | 1910.94] loss=3.56 avg=2.63\n",
            "[2180 | 1919.64] loss=2.10 avg=2.63\n",
            "[2190 | 1928.34] loss=2.24 avg=2.62\n",
            "[2200 | 1937.04] loss=3.27 avg=2.63\n",
            "[2210 | 1945.72] loss=2.52 avg=2.63\n",
            "[2220 | 1954.39] loss=3.51 avg=2.64\n",
            "[2230 | 1963.06] loss=2.34 avg=2.63\n",
            "[2240 | 1971.74] loss=3.10 avg=2.64\n",
            "[2250 | 1980.43] loss=2.99 avg=2.64\n",
            "[2260 | 1989.11] loss=2.41 avg=2.64\n",
            "[2270 | 1997.81] loss=2.41 avg=2.64\n",
            "[2280 | 2006.51] loss=2.93 avg=2.64\n",
            "[2290 | 2015.18] loss=2.51 avg=2.64\n",
            "[2300 | 2023.86] loss=2.53 avg=2.64\n",
            "[2310 | 2032.55] loss=2.60 avg=2.64\n",
            "[2320 | 2041.22] loss=2.64 avg=2.64\n",
            "[2330 | 2049.90] loss=1.68 avg=2.63\n",
            "[2340 | 2058.60] loss=2.81 avg=2.63\n",
            "[2350 | 2067.29] loss=2.33 avg=2.63\n",
            "[2360 | 2075.96] loss=2.86 avg=2.63\n",
            "[2370 | 2084.67] loss=2.83 avg=2.63\n",
            "[2380 | 2093.34] loss=2.54 avg=2.63\n",
            "[2390 | 2102.02] loss=3.14 avg=2.64\n",
            "[2400 | 2110.71] loss=2.17 avg=2.63\n",
            "[2410 | 2119.42] loss=2.58 avg=2.63\n",
            "[2420 | 2128.12] loss=2.67 avg=2.63\n",
            "[2430 | 2136.80] loss=3.52 avg=2.64\n",
            "[2440 | 2145.49] loss=2.05 avg=2.63\n",
            "[2450 | 2154.17] loss=2.81 avg=2.64\n",
            "[2460 | 2162.87] loss=1.59 avg=2.62\n",
            "[2470 | 2171.61] loss=3.00 avg=2.63\n",
            "[2480 | 2180.33] loss=1.09 avg=2.61\n",
            "[2490 | 2189.03] loss=1.74 avg=2.60\n",
            "[2500 | 2197.73] loss=1.77 avg=2.59\n",
            "[2510 | 2206.43] loss=2.33 avg=2.59\n",
            "[2520 | 2215.11] loss=1.90 avg=2.58\n",
            "[2530 | 2223.79] loss=1.77 avg=2.57\n",
            "[2540 | 2232.48] loss=3.11 avg=2.58\n",
            "[2550 | 2241.17] loss=2.14 avg=2.57\n",
            "[2560 | 2249.85] loss=1.52 avg=2.56\n",
            "[2570 | 2258.55] loss=1.57 avg=2.55\n",
            "[2580 | 2267.22] loss=3.46 avg=2.56\n",
            "[2590 | 2275.90] loss=2.16 avg=2.56\n",
            "[2600 | 2284.59] loss=2.55 avg=2.56\n",
            "[2610 | 2293.28] loss=1.78 avg=2.55\n",
            "[2620 | 2301.95] loss=0.91 avg=2.53\n",
            "[2630 | 2310.65] loss=2.28 avg=2.53\n",
            "[2640 | 2319.33] loss=3.24 avg=2.54\n",
            "[2650 | 2328.04] loss=2.59 avg=2.54\n",
            "[2660 | 2336.71] loss=1.80 avg=2.53\n",
            "[2670 | 2345.40] loss=2.37 avg=2.53\n",
            "[2680 | 2354.10] loss=2.74 avg=2.53\n",
            "[2690 | 2362.78] loss=2.45 avg=2.53\n",
            "[2700 | 2371.48] loss=1.43 avg=2.52\n",
            "[2710 | 2380.19] loss=2.65 avg=2.52\n",
            "[2720 | 2388.90] loss=1.99 avg=2.51\n",
            "[2730 | 2397.60] loss=1.89 avg=2.51\n",
            "[2740 | 2406.30] loss=3.03 avg=2.51\n",
            "[2750 | 2414.99] loss=3.28 avg=2.52\n",
            "[2760 | 2423.68] loss=1.80 avg=2.51\n",
            "[2770 | 2432.39] loss=3.48 avg=2.52\n",
            "[2780 | 2441.07] loss=1.83 avg=2.52\n",
            "[2790 | 2449.77] loss=2.37 avg=2.51\n",
            "[2800 | 2458.45] loss=3.22 avg=2.52\n",
            "[2810 | 2467.14] loss=1.66 avg=2.51\n",
            "[2820 | 2475.86] loss=2.84 avg=2.52\n",
            "[2830 | 2484.55] loss=3.29 avg=2.52\n",
            "[2840 | 2493.26] loss=1.99 avg=2.52\n",
            "[2850 | 2501.93] loss=1.90 avg=2.51\n",
            "[2860 | 2510.62] loss=2.00 avg=2.51\n",
            "[2870 | 2519.34] loss=2.40 avg=2.51\n",
            "[2880 | 2528.04] loss=2.54 avg=2.51\n",
            "[2890 | 2536.74] loss=2.19 avg=2.50\n",
            "[2900 | 2545.43] loss=2.54 avg=2.50\n",
            "[2910 | 2554.12] loss=2.66 avg=2.50\n",
            "[2920 | 2562.78] loss=2.56 avg=2.51\n",
            "[2930 | 2571.46] loss=2.15 avg=2.50\n",
            "[2940 | 2580.12] loss=2.18 avg=2.50\n",
            "[2950 | 2588.83] loss=2.67 avg=2.50\n",
            "[2960 | 2597.52] loss=1.25 avg=2.49\n",
            "[2970 | 2606.17] loss=2.45 avg=2.49\n",
            "[2980 | 2614.86] loss=1.61 avg=2.48\n",
            "[2990 | 2623.54] loss=3.17 avg=2.48\n",
            "[3000 | 2632.23] loss=2.99 avg=2.49\n",
            "Saving checkpoint/run42K/model-3000\n",
            "[3010 | 2648.18] loss=2.45 avg=2.49\n",
            "[3020 | 2656.86] loss=1.84 avg=2.48\n",
            "[3030 | 2665.58] loss=1.64 avg=2.47\n",
            "[3040 | 2674.26] loss=2.77 avg=2.48\n",
            "[3050 | 2682.92] loss=2.46 avg=2.48\n",
            "[3060 | 2691.61] loss=2.11 avg=2.47\n",
            "[3070 | 2700.29] loss=0.73 avg=2.45\n",
            "[3080 | 2708.97] loss=3.07 avg=2.46\n",
            "[3090 | 2717.64] loss=2.81 avg=2.46\n",
            "[3100 | 2726.36] loss=2.69 avg=2.47\n",
            "[3110 | 2735.02] loss=2.67 avg=2.47\n",
            "[3120 | 2743.69] loss=3.43 avg=2.48\n",
            "[3130 | 2752.37] loss=2.21 avg=2.48\n",
            "[3140 | 2761.09] loss=2.98 avg=2.48\n",
            "[3150 | 2769.79] loss=2.60 avg=2.48\n",
            "[3160 | 2778.46] loss=2.91 avg=2.49\n",
            "[3170 | 2787.17] loss=2.41 avg=2.49\n",
            "[3180 | 2795.86] loss=2.22 avg=2.48\n",
            "[3190 | 2804.54] loss=1.96 avg=2.48\n",
            "[3200 | 2813.22] loss=2.59 avg=2.48\n",
            "[3210 | 2821.90] loss=2.70 avg=2.48\n",
            "[3220 | 2830.59] loss=2.32 avg=2.48\n",
            "[3230 | 2839.29] loss=2.81 avg=2.48\n",
            "[3240 | 2847.97] loss=1.29 avg=2.47\n",
            "[3250 | 2856.66] loss=3.24 avg=2.48\n",
            "[3260 | 2865.35] loss=2.26 avg=2.48\n",
            "[3270 | 2874.03] loss=2.50 avg=2.48\n",
            "[3280 | 2882.71] loss=2.44 avg=2.48\n",
            "[3290 | 2891.42] loss=1.97 avg=2.47\n",
            "[3300 | 2900.10] loss=2.20 avg=2.47\n",
            "[3310 | 2908.78] loss=2.15 avg=2.47\n",
            "[3320 | 2917.48] loss=1.77 avg=2.46\n",
            "[3330 | 2926.15] loss=2.67 avg=2.46\n",
            "[3340 | 2934.83] loss=1.95 avg=2.45\n",
            "[3350 | 2943.49] loss=2.50 avg=2.46\n",
            "[3360 | 2952.19] loss=1.84 avg=2.45\n",
            "[3370 | 2960.91] loss=1.97 avg=2.44\n",
            "[3380 | 2969.59] loss=2.55 avg=2.45\n",
            "[3390 | 2978.31] loss=1.70 avg=2.44\n",
            "[3400 | 2986.98] loss=1.94 avg=2.43\n",
            "[3410 | 2995.67] loss=2.70 avg=2.43\n",
            "[3420 | 3004.35] loss=2.57 avg=2.44\n",
            "[3430 | 3013.06] loss=1.25 avg=2.42\n",
            "[3440 | 3021.76] loss=2.24 avg=2.42\n",
            "[3450 | 3030.44] loss=2.22 avg=2.42\n",
            "[3460 | 3039.13] loss=1.37 avg=2.41\n",
            "[3470 | 3047.83] loss=1.38 avg=2.40\n",
            "[3480 | 3056.51] loss=2.23 avg=2.40\n",
            "[3490 | 3065.22] loss=2.06 avg=2.39\n",
            "[3500 | 3073.89] loss=0.94 avg=2.38\n",
            "[3510 | 3082.57] loss=3.21 avg=2.39\n",
            "[3520 | 3091.28] loss=1.80 avg=2.38\n",
            "[3530 | 3099.97] loss=1.95 avg=2.38\n",
            "[3540 | 3108.64] loss=1.63 avg=2.37\n",
            "[3550 | 3117.34] loss=1.98 avg=2.36\n",
            "[3560 | 3126.02] loss=2.28 avg=2.36\n",
            "[3570 | 3134.72] loss=1.31 avg=2.35\n",
            "[3580 | 3143.45] loss=2.86 avg=2.36\n",
            "[3590 | 3152.15] loss=2.23 avg=2.36\n",
            "[3600 | 3160.83] loss=2.32 avg=2.36\n",
            "[3610 | 3169.50] loss=1.62 avg=2.35\n",
            "[3620 | 3178.18] loss=2.63 avg=2.35\n",
            "[3630 | 3186.89] loss=1.49 avg=2.34\n",
            "[3640 | 3195.61] loss=1.19 avg=2.33\n",
            "[3650 | 3204.30] loss=2.01 avg=2.33\n",
            "[3660 | 3212.99] loss=2.16 avg=2.33\n",
            "[3670 | 3221.67] loss=2.11 avg=2.32\n",
            "[3680 | 3230.36] loss=2.38 avg=2.32\n",
            "[3690 | 3239.06] loss=1.40 avg=2.32\n",
            "[3700 | 3247.75] loss=2.77 avg=2.32\n",
            "[3710 | 3256.41] loss=2.39 avg=2.32\n",
            "[3720 | 3265.11] loss=1.70 avg=2.31\n",
            "[3730 | 3273.79] loss=2.61 avg=2.32\n",
            "[3740 | 3282.48] loss=1.42 avg=2.31\n",
            "[3750 | 3291.17] loss=2.49 avg=2.31\n",
            "[3760 | 3299.86] loss=1.91 avg=2.31\n",
            "[3770 | 3308.55] loss=2.57 avg=2.31\n",
            "[3780 | 3317.24] loss=2.60 avg=2.31\n",
            "[3790 | 3325.93] loss=1.72 avg=2.31\n",
            "[3800 | 3334.64] loss=2.85 avg=2.31\n",
            "[3810 | 3343.33] loss=2.59 avg=2.31\n",
            "[3820 | 3352.01] loss=2.08 avg=2.31\n",
            "[3830 | 3360.71] loss=1.30 avg=2.30\n",
            "[3840 | 3369.39] loss=3.00 avg=2.31\n",
            "[3850 | 3378.09] loss=1.75 avg=2.30\n",
            "[3860 | 3386.77] loss=2.43 avg=2.30\n",
            "[3870 | 3395.47] loss=0.64 avg=2.29\n",
            "[3890 | 3412.87] loss=2.63 avg=2.29\n",
            "[3900 | 3421.55] loss=2.28 avg=2.29\n",
            "[3910 | 3430.23] loss=1.57 avg=2.28\n",
            "[3920 | 3438.93] loss=1.95 avg=2.28\n",
            "[3930 | 3447.64] loss=1.66 avg=2.27\n",
            "[3940 | 3456.34] loss=1.43 avg=2.26\n",
            "[3950 | 3465.03] loss=1.79 avg=2.26\n",
            "[3960 | 3473.72] loss=0.71 avg=2.24\n",
            "[3970 | 3482.41] loss=2.24 avg=2.24\n",
            "[3980 | 3491.10] loss=2.25 avg=2.24\n",
            "[3990 | 3499.79] loss=1.95 avg=2.24\n",
            "[4000 | 3508.48] loss=2.76 avg=2.24\n",
            "Saving checkpoint/run42K/model-4000\n",
            "[4010 | 3524.44] loss=1.11 avg=2.23\n",
            "[4020 | 3533.12] loss=2.72 avg=2.24\n",
            "[4030 | 3541.80] loss=1.91 avg=2.23\n",
            "[4040 | 3550.51] loss=0.79 avg=2.22\n",
            "[4050 | 3559.20] loss=2.37 avg=2.22\n",
            "[4060 | 3567.92] loss=1.95 avg=2.22\n",
            "[4070 | 3576.61] loss=2.42 avg=2.22\n",
            "[4080 | 3585.28] loss=2.50 avg=2.22\n",
            "[4090 | 3593.94] loss=1.91 avg=2.22\n",
            "[4100 | 3602.63] loss=2.15 avg=2.22\n",
            "[4110 | 3611.30] loss=2.54 avg=2.22\n",
            "[4120 | 3619.99] loss=0.88 avg=2.21\n",
            "[4130 | 3628.70] loss=1.94 avg=2.21\n",
            "[4140 | 3637.39] loss=2.27 avg=2.21\n",
            "[4150 | 3646.07] loss=1.72 avg=2.20\n",
            "[4160 | 3654.73] loss=2.16 avg=2.20\n",
            "[4170 | 3663.46] loss=2.56 avg=2.21\n",
            "[4180 | 3672.13] loss=1.41 avg=2.20\n",
            "[4190 | 3680.82] loss=1.71 avg=2.19\n",
            "[4200 | 3689.49] loss=2.76 avg=2.20\n",
            "[4210 | 3698.19] loss=1.99 avg=2.20\n",
            "[4220 | 3706.88] loss=2.19 avg=2.20\n",
            "[4230 | 3715.58] loss=1.89 avg=2.19\n",
            "[4240 | 3724.27] loss=0.95 avg=2.18\n",
            "[4250 | 3732.95] loss=2.01 avg=2.18\n",
            "[4260 | 3741.64] loss=1.14 avg=2.17\n",
            "[4270 | 3750.33] loss=1.60 avg=2.16\n",
            "[4280 | 3759.03] loss=2.35 avg=2.16\n",
            "[4290 | 3767.74] loss=2.60 avg=2.17\n",
            "[4300 | 3776.44] loss=1.27 avg=2.16\n",
            "[4310 | 3785.12] loss=1.26 avg=2.15\n",
            "[4320 | 3793.81] loss=1.87 avg=2.15\n",
            "[4330 | 3802.50] loss=2.30 avg=2.15\n",
            "[4340 | 3811.21] loss=1.22 avg=2.14\n",
            "[4350 | 3819.91] loss=2.28 avg=2.14\n",
            "[4360 | 3828.62] loss=1.71 avg=2.14\n",
            "[4370 | 3837.31] loss=1.98 avg=2.13\n",
            "[4380 | 3845.98] loss=2.31 avg=2.14\n",
            "[4390 | 3854.68] loss=2.56 avg=2.14\n",
            "[4400 | 3863.36] loss=1.84 avg=2.14\n",
            "[4410 | 3872.07] loss=2.12 avg=2.14\n",
            "[4420 | 3880.75] loss=1.94 avg=2.14\n",
            "[4430 | 3889.45] loss=1.15 avg=2.13\n",
            "[4440 | 3898.15] loss=1.51 avg=2.12\n",
            "[4450 | 3906.85] loss=2.34 avg=2.12\n",
            "[4460 | 3915.55] loss=2.17 avg=2.12\n",
            "[4470 | 3924.27] loss=2.46 avg=2.13\n",
            "[4480 | 3932.94] loss=1.75 avg=2.12\n",
            "[4490 | 3941.63] loss=0.82 avg=2.11\n",
            "[4500 | 3950.32] loss=1.34 avg=2.10\n",
            "[4510 | 3959.00] loss=3.06 avg=2.11\n",
            "[4520 | 3967.68] loss=1.95 avg=2.11\n",
            "[4530 | 3976.36] loss=2.60 avg=2.11\n",
            "[4540 | 3985.04] loss=1.77 avg=2.11\n",
            "[4550 | 3993.73] loss=2.61 avg=2.12\n",
            "[4560 | 4002.44] loss=2.76 avg=2.12\n",
            "[4570 | 4011.14] loss=2.70 avg=2.13\n",
            "[4580 | 4019.83] loss=1.71 avg=2.12\n",
            "[4590 | 4028.53] loss=1.52 avg=2.12\n",
            "[4600 | 4037.22] loss=2.81 avg=2.12\n",
            "[4610 | 4045.91] loss=0.95 avg=2.11\n",
            "[4620 | 4054.58] loss=1.99 avg=2.11\n",
            "[4630 | 4063.28] loss=1.44 avg=2.10\n",
            "[4640 | 4071.97] loss=2.38 avg=2.11\n",
            "[4650 | 4080.68] loss=1.73 avg=2.10\n",
            "[4660 | 4089.36] loss=1.88 avg=2.10\n",
            "[4670 | 4098.05] loss=1.01 avg=2.09\n",
            "[4680 | 4106.71] loss=1.41 avg=2.08\n",
            "[4690 | 4115.40] loss=2.68 avg=2.09\n",
            "[4700 | 4124.08] loss=2.41 avg=2.09\n",
            "[4710 | 4132.77] loss=1.10 avg=2.08\n",
            "[4720 | 4141.45] loss=2.17 avg=2.08\n",
            "[4730 | 4150.15] loss=1.43 avg=2.08\n",
            "[4740 | 4158.84] loss=2.02 avg=2.08\n",
            "[4750 | 4167.54] loss=1.89 avg=2.07\n",
            "[4760 | 4176.25] loss=2.76 avg=2.08\n",
            "[4770 | 4184.94] loss=1.20 avg=2.07\n",
            "[4780 | 4193.66] loss=1.33 avg=2.07\n",
            "[4790 | 4202.36] loss=1.73 avg=2.06\n",
            "[4800 | 4211.05] loss=2.16 avg=2.06\n",
            "[4810 | 4219.75] loss=1.66 avg=2.06\n",
            "[4820 | 4228.49] loss=1.13 avg=2.05\n",
            "[4830 | 4237.20] loss=3.63 avg=2.07\n",
            "[4840 | 4245.88] loss=1.49 avg=2.06\n",
            "[4850 | 4254.57] loss=1.72 avg=2.06\n",
            "[4860 | 4263.25] loss=2.90 avg=2.06\n",
            "[4870 | 4271.96] loss=1.77 avg=2.06\n",
            "[4880 | 4280.67] loss=1.39 avg=2.05\n",
            "[4890 | 4289.35] loss=1.55 avg=2.05\n",
            "[4900 | 4298.07] loss=0.19 avg=2.03\n",
            "[4910 | 4306.79] loss=1.85 avg=2.03\n",
            "[4920 | 4315.50] loss=2.80 avg=2.04\n",
            "[4930 | 4324.18] loss=1.34 avg=2.03\n",
            "[4940 | 4332.87] loss=1.17 avg=2.02\n",
            "[4950 | 4341.56] loss=1.36 avg=2.01\n",
            "[4960 | 4350.25] loss=1.04 avg=2.00\n",
            "[4970 | 4358.96] loss=0.43 avg=1.99\n",
            "[4980 | 4367.65] loss=1.13 avg=1.98\n",
            "[4990 | 4376.36] loss=0.97 avg=1.97\n",
            "[5000 | 4385.06] loss=2.20 avg=1.97\n",
            "Saving checkpoint/run42K/model-5000\n",
            "[5010 | 4401.07] loss=1.05 avg=1.96\n",
            "[5020 | 4409.74] loss=0.86 avg=1.95\n",
            "[5030 | 4418.44] loss=2.10 avg=1.95\n",
            "[5040 | 4427.13] loss=2.44 avg=1.96\n",
            "[5050 | 4435.87] loss=1.54 avg=1.95\n",
            "[5060 | 4444.58] loss=1.50 avg=1.95\n",
            "[5070 | 4453.28] loss=0.83 avg=1.94\n",
            "[5080 | 4461.97] loss=1.92 avg=1.94\n",
            "[5090 | 4470.64] loss=2.02 avg=1.94\n",
            "[5100 | 4479.34] loss=2.29 avg=1.94\n",
            "[5110 | 4488.04] loss=3.16 avg=1.95\n",
            "[5120 | 4496.73] loss=1.64 avg=1.95\n",
            "[5130 | 4505.40] loss=2.13 avg=1.95\n",
            "[5140 | 4514.09] loss=1.27 avg=1.95\n",
            "[5150 | 4522.78] loss=2.80 avg=1.96\n",
            "[5160 | 4531.48] loss=2.98 avg=1.97\n",
            "[5170 | 4540.17] loss=0.49 avg=1.95\n",
            "[5180 | 4548.85] loss=0.92 avg=1.94\n",
            "[5190 | 4557.54] loss=1.45 avg=1.94\n",
            "[5200 | 4566.22] loss=1.28 avg=1.93\n",
            "[5210 | 4574.91] loss=1.31 avg=1.92\n",
            "[5220 | 4583.59] loss=1.49 avg=1.92\n",
            "[5230 | 4592.30] loss=1.55 avg=1.91\n",
            "[5240 | 4601.01] loss=2.29 avg=1.92\n",
            "[5250 | 4609.71] loss=1.75 avg=1.92\n",
            "[5260 | 4618.40] loss=2.47 avg=1.92\n",
            "[5270 | 4627.08] loss=2.19 avg=1.92\n",
            "[5280 | 4635.78] loss=1.26 avg=1.92\n",
            "[5290 | 4644.49] loss=1.16 avg=1.91\n",
            "[5300 | 4653.14] loss=1.77 avg=1.91\n",
            "[5310 | 4661.82] loss=0.68 avg=1.90\n",
            "[5320 | 4670.50] loss=1.21 avg=1.89\n",
            "[5330 | 4679.21] loss=2.07 avg=1.89\n",
            "[5340 | 4687.88] loss=1.09 avg=1.88\n",
            "[5350 | 4696.57] loss=0.89 avg=1.87\n",
            "[5360 | 4705.23] loss=2.13 avg=1.88\n",
            "[5370 | 4713.92] loss=1.62 avg=1.87\n",
            "[5380 | 4722.62] loss=2.13 avg=1.88\n",
            "[5390 | 4731.31] loss=2.30 avg=1.88\n",
            "[5400 | 4740.02] loss=1.91 avg=1.88\n",
            "[5410 | 4748.69] loss=1.15 avg=1.87\n",
            "[5420 | 4757.36] loss=1.30 avg=1.87\n",
            "[5430 | 4766.03] loss=1.11 avg=1.86\n",
            "[5440 | 4774.68] loss=2.10 avg=1.86\n",
            "[5450 | 4783.41] loss=1.94 avg=1.86\n",
            "[5460 | 4792.07] loss=1.40 avg=1.86\n",
            "[5470 | 4800.78] loss=3.22 avg=1.87\n",
            "[5480 | 4809.45] loss=1.31 avg=1.87\n",
            "[5490 | 4818.13] loss=2.30 avg=1.87\n",
            "[5500 | 4826.82] loss=1.59 avg=1.87\n",
            "[5510 | 4835.54] loss=2.13 avg=1.87\n",
            "[5520 | 4844.24] loss=1.62 avg=1.87\n",
            "[5530 | 4852.94] loss=1.06 avg=1.86\n",
            "[5540 | 4861.60] loss=1.04 avg=1.85\n",
            "[5550 | 4870.28] loss=1.46 avg=1.85\n",
            "[5560 | 4878.95] loss=1.25 avg=1.84\n",
            "[5570 | 4887.63] loss=3.19 avg=1.86\n",
            "[5580 | 4896.31] loss=1.57 avg=1.85\n",
            "[5590 | 4904.98] loss=1.76 avg=1.85\n",
            "[5600 | 4913.66] loss=2.72 avg=1.86\n",
            "[5610 | 4922.37] loss=1.58 avg=1.86\n",
            "[5620 | 4931.05] loss=1.84 avg=1.86\n",
            "[5630 | 4939.74] loss=2.93 avg=1.87\n",
            "[5640 | 4948.42] loss=1.01 avg=1.86\n",
            "[5650 | 4957.14] loss=2.23 avg=1.86\n",
            "[5660 | 4965.83] loss=1.25 avg=1.86\n",
            "[5670 | 4974.53] loss=0.71 avg=1.85\n",
            "[5680 | 4983.21] loss=2.11 avg=1.85\n",
            "[5690 | 4991.92] loss=1.53 avg=1.85\n",
            "[5700 | 5000.64] loss=2.67 avg=1.85\n",
            "[5710 | 5009.35] loss=1.23 avg=1.85\n",
            "[5720 | 5018.04] loss=1.02 avg=1.84\n",
            "[5730 | 5026.74] loss=2.55 avg=1.85\n",
            "[5740 | 5035.42] loss=2.26 avg=1.85\n",
            "[5750 | 5044.10] loss=0.77 avg=1.84\n",
            "[5760 | 5052.78] loss=0.75 avg=1.83\n",
            "[5770 | 5061.48] loss=0.79 avg=1.82\n",
            "[5780 | 5070.18] loss=1.23 avg=1.81\n",
            "[5790 | 5078.87] loss=1.46 avg=1.81\n",
            "[5800 | 5087.56] loss=1.39 avg=1.80\n",
            "[5810 | 5096.22] loss=1.14 avg=1.80\n",
            "[5820 | 5104.95] loss=1.53 avg=1.79\n",
            "[5830 | 5113.65] loss=0.91 avg=1.79\n",
            "[5840 | 5122.33] loss=0.86 avg=1.78\n",
            "[5850 | 5131.00] loss=0.71 avg=1.77\n",
            "[5860 | 5139.69] loss=0.73 avg=1.76\n",
            "[5870 | 5148.37] loss=2.17 avg=1.76\n",
            "[5880 | 5157.06] loss=2.22 avg=1.76\n",
            "[5890 | 5165.74] loss=1.15 avg=1.76\n",
            "[5900 | 5174.43] loss=0.34 avg=1.74\n",
            "[5910 | 5183.12] loss=2.20 avg=1.75\n",
            "[5920 | 5191.81] loss=1.85 avg=1.75\n",
            "[5930 | 5200.50] loss=1.94 avg=1.75\n",
            "[5940 | 5209.18] loss=1.02 avg=1.74\n",
            "[5950 | 5217.85] loss=3.26 avg=1.76\n",
            "[5960 | 5226.54] loss=1.31 avg=1.75\n",
            "[5970 | 5235.25] loss=1.37 avg=1.75\n",
            "[5980 | 5243.92] loss=0.86 avg=1.74\n",
            "[5990 | 5252.62] loss=1.16 avg=1.74\n",
            "[6000 | 5261.30] loss=1.30 avg=1.73\n",
            "Saving checkpoint/run42K/model-6000\n",
            "[6020 | 5286.18] loss=1.00 avg=1.72\n",
            "[6030 | 5294.87] loss=2.50 avg=1.72\n",
            "[6040 | 5303.55] loss=1.30 avg=1.72\n",
            "[6050 | 5312.26] loss=2.39 avg=1.73\n",
            "[6060 | 5320.96] loss=1.21 avg=1.72\n",
            "[6070 | 5329.64] loss=1.54 avg=1.72\n",
            "[6080 | 5338.35] loss=2.17 avg=1.72\n",
            "[6090 | 5347.04] loss=1.10 avg=1.72\n",
            "[6100 | 5355.74] loss=2.39 avg=1.72\n",
            "[6110 | 5364.43] loss=1.78 avg=1.73\n",
            "[6120 | 5373.13] loss=1.64 avg=1.72\n",
            "[6130 | 5381.83] loss=1.63 avg=1.72\n",
            "[6140 | 5390.50] loss=0.79 avg=1.71\n",
            "[6150 | 5399.18] loss=1.56 avg=1.71\n",
            "[6160 | 5407.87] loss=2.29 avg=1.72\n",
            "[6170 | 5416.59] loss=1.52 avg=1.72\n",
            "[6180 | 5425.28] loss=1.64 avg=1.72\n",
            "[6190 | 5433.97] loss=1.32 avg=1.71\n",
            "[6200 | 5442.65] loss=0.76 avg=1.70\n",
            "[6210 | 5451.36] loss=1.30 avg=1.70\n",
            "[6220 | 5460.06] loss=1.92 avg=1.70\n",
            "[6230 | 5468.76] loss=1.82 avg=1.70\n",
            "[6240 | 5477.45] loss=1.30 avg=1.70\n",
            "[6250 | 5486.15] loss=0.96 avg=1.69\n",
            "[6260 | 5494.84] loss=3.39 avg=1.71\n",
            "[6270 | 5503.55] loss=0.61 avg=1.70\n",
            "[6280 | 5512.22] loss=1.13 avg=1.69\n",
            "[6290 | 5520.94] loss=0.96 avg=1.68\n",
            "[6300 | 5529.63] loss=2.09 avg=1.69\n",
            "[6310 | 5538.35] loss=1.24 avg=1.68\n",
            "[6320 | 5547.05] loss=1.68 avg=1.68\n",
            "[6330 | 5555.74] loss=2.23 avg=1.69\n",
            "[6340 | 5564.46] loss=2.49 avg=1.70\n",
            "[6350 | 5573.14] loss=2.28 avg=1.70\n",
            "[6360 | 5581.83] loss=1.84 avg=1.70\n",
            "[6370 | 5590.52] loss=0.68 avg=1.69\n",
            "[6380 | 5599.21] loss=0.80 avg=1.68\n",
            "[6390 | 5607.89] loss=0.55 avg=1.67\n",
            "[6400 | 5616.57] loss=0.75 avg=1.66\n",
            "[6410 | 5625.26] loss=0.34 avg=1.65\n",
            "[6420 | 5633.99] loss=0.85 avg=1.64\n",
            "[6430 | 5642.67] loss=2.08 avg=1.65\n",
            "[6440 | 5651.37] loss=1.25 avg=1.64\n",
            "[6450 | 5660.05] loss=1.73 avg=1.64\n",
            "[6460 | 5668.77] loss=1.82 avg=1.65\n",
            "[6470 | 5677.48] loss=0.36 avg=1.63\n",
            "[6480 | 5686.16] loss=1.02 avg=1.63\n",
            "[6490 | 5694.85] loss=1.95 avg=1.63\n",
            "[6500 | 5703.54] loss=0.46 avg=1.62\n",
            "[6510 | 5712.23] loss=0.75 avg=1.61\n",
            "[6520 | 5720.92] loss=1.98 avg=1.61\n",
            "[6530 | 5729.61] loss=2.39 avg=1.62\n",
            "[6540 | 5738.29] loss=2.14 avg=1.63\n",
            "[6550 | 5747.00] loss=2.63 avg=1.64\n",
            "[6560 | 5755.70] loss=0.46 avg=1.62\n",
            "[6570 | 5764.38] loss=0.33 avg=1.61\n",
            "[6580 | 5773.07] loss=0.93 avg=1.60\n",
            "[6590 | 5781.76] loss=0.48 avg=1.59\n",
            "[6600 | 5790.42] loss=1.61 avg=1.59\n",
            "[6610 | 5799.11] loss=2.29 avg=1.60\n",
            "[6620 | 5807.79] loss=3.27 avg=1.62\n",
            "[6630 | 5816.46] loss=0.85 avg=1.61\n",
            "[6640 | 5825.13] loss=0.65 avg=1.60\n",
            "[6650 | 5833.80] loss=0.57 avg=1.59\n",
            "[6660 | 5842.50] loss=1.02 avg=1.58\n",
            "[6670 | 5851.17] loss=1.26 avg=1.58\n",
            "[6680 | 5859.88] loss=0.72 avg=1.57\n",
            "[6690 | 5868.59] loss=3.47 avg=1.59\n",
            "[6700 | 5877.27] loss=0.67 avg=1.58\n",
            "[6710 | 5885.94] loss=0.58 avg=1.57\n",
            "[6720 | 5894.64] loss=0.99 avg=1.57\n",
            "[6730 | 5903.34] loss=1.76 avg=1.57\n",
            "[6740 | 5912.02] loss=1.21 avg=1.56\n",
            "[6750 | 5920.70] loss=1.66 avg=1.57\n",
            "[6760 | 5929.39] loss=1.28 avg=1.56\n",
            "[6770 | 5938.15] loss=1.49 avg=1.56\n",
            "[6780 | 5946.84] loss=0.51 avg=1.55\n",
            "[6790 | 5955.53] loss=0.85 avg=1.54\n",
            "[6800 | 5964.21] loss=2.47 avg=1.55\n",
            "[6810 | 5972.92] loss=0.59 avg=1.54\n",
            "[6820 | 5981.61] loss=1.12 avg=1.54\n",
            "[6830 | 5990.30] loss=0.34 avg=1.53\n",
            "[6840 | 5998.97] loss=0.37 avg=1.52\n",
            "[6850 | 6007.65] loss=2.71 avg=1.53\n",
            "[6860 | 6016.35] loss=1.40 avg=1.53\n",
            "[6870 | 6025.05] loss=1.06 avg=1.52\n",
            "[6880 | 6033.73] loss=2.02 avg=1.53\n",
            "[6890 | 6042.44] loss=0.59 avg=1.52\n",
            "[6900 | 6051.14] loss=0.28 avg=1.50\n",
            "[6910 | 6059.82] loss=1.95 avg=1.51\n",
            "[6920 | 6068.50] loss=1.30 avg=1.51\n",
            "[6930 | 6077.18] loss=0.86 avg=1.50\n",
            "[6940 | 6085.89] loss=0.74 avg=1.49\n",
            "[6950 | 6094.56] loss=1.49 avg=1.49\n",
            "[6960 | 6103.25] loss=2.21 avg=1.50\n",
            "[6970 | 6111.94] loss=0.99 avg=1.50\n",
            "[6980 | 6120.65] loss=1.35 avg=1.49\n",
            "[6990 | 6129.36] loss=0.21 avg=1.48\n",
            "[7000 | 6138.05] loss=0.83 avg=1.47\n",
            "Saving checkpoint/run42K/model-7000\n",
            "[7010 | 6154.13] loss=2.60 avg=1.49\n",
            "[7020 | 6162.82] loss=0.19 avg=1.47\n",
            "[7030 | 6171.49] loss=1.33 avg=1.47\n",
            "[7040 | 6180.19] loss=0.99 avg=1.47\n",
            "[7050 | 6188.89] loss=0.60 avg=1.46\n",
            "[7060 | 6197.57] loss=0.66 avg=1.45\n",
            "[7070 | 6206.26] loss=3.56 avg=1.47\n",
            "[7080 | 6214.95] loss=0.83 avg=1.46\n",
            "[7090 | 6223.66] loss=0.28 avg=1.45\n",
            "[7100 | 6232.36] loss=0.57 avg=1.44\n",
            "[7110 | 6241.06] loss=1.54 avg=1.44\n",
            "[7120 | 6249.80] loss=1.39 avg=1.44\n",
            "[7130 | 6258.49] loss=1.58 avg=1.45\n",
            "[7140 | 6267.20] loss=0.12 avg=1.43\n",
            "[7150 | 6275.90] loss=1.18 avg=1.43\n",
            "[7160 | 6284.58] loss=3.04 avg=1.45\n",
            "[7170 | 6293.26] loss=0.97 avg=1.44\n",
            "[7180 | 6301.95] loss=1.30 avg=1.44\n",
            "[7190 | 6310.66] loss=0.67 avg=1.43\n",
            "[7200 | 6319.36] loss=2.38 avg=1.44\n",
            "[7210 | 6328.04] loss=2.45 avg=1.45\n",
            "[7220 | 6336.72] loss=1.10 avg=1.45\n",
            "[7230 | 6345.40] loss=2.48 avg=1.46\n",
            "[7240 | 6354.07] loss=2.12 avg=1.47\n",
            "[7250 | 6362.76] loss=0.94 avg=1.46\n",
            "[7260 | 6371.45] loss=2.31 avg=1.47\n",
            "[7270 | 6380.14] loss=0.95 avg=1.46\n",
            "[7280 | 6388.85] loss=1.01 avg=1.46\n",
            "[7290 | 6397.54] loss=0.26 avg=1.45\n",
            "[7300 | 6406.24] loss=2.68 avg=1.46\n",
            "[7310 | 6414.93] loss=0.54 avg=1.45\n",
            "[7320 | 6423.63] loss=1.74 avg=1.45\n",
            "[7340 | 6441.01] loss=3.19 avg=1.47\n",
            "[7350 | 6449.73] loss=1.24 avg=1.46\n",
            "[7360 | 6458.41] loss=1.19 avg=1.46\n",
            "[7370 | 6467.09] loss=0.59 avg=1.45\n",
            "[7380 | 6475.76] loss=3.57 avg=1.47\n",
            "[7390 | 6484.45] loss=0.75 avg=1.47\n",
            "[7400 | 6493.13] loss=0.27 avg=1.45\n",
            "[7410 | 6501.82] loss=2.38 avg=1.46\n",
            "[7420 | 6510.51] loss=1.38 avg=1.46\n",
            "[7430 | 6519.22] loss=2.29 avg=1.47\n",
            "[7440 | 6527.90] loss=0.56 avg=1.46\n",
            "[7450 | 6536.59] loss=0.22 avg=1.45\n",
            "[7460 | 6545.28] loss=0.72 avg=1.44\n",
            "[7470 | 6553.99] loss=1.74 avg=1.45\n",
            "[7480 | 6562.71] loss=1.40 avg=1.45\n",
            "[7490 | 6571.40] loss=1.30 avg=1.44\n",
            "[7500 | 6580.10] loss=0.79 avg=1.44\n",
            "[7510 | 6588.78] loss=1.77 avg=1.44\n",
            "[7520 | 6597.48] loss=0.53 avg=1.43\n",
            "[7530 | 6606.21] loss=0.84 avg=1.43\n",
            "[7540 | 6614.89] loss=0.47 avg=1.42\n",
            "[7550 | 6623.58] loss=0.20 avg=1.40\n",
            "[7560 | 6632.26] loss=1.98 avg=1.41\n",
            "[7570 | 6640.93] loss=2.66 avg=1.42\n",
            "[7580 | 6649.62] loss=0.63 avg=1.41\n",
            "[7590 | 6658.30] loss=0.85 avg=1.41\n",
            "[7600 | 6667.01] loss=1.41 avg=1.41\n",
            "[7610 | 6675.69] loss=1.24 avg=1.41\n",
            "[7620 | 6684.36] loss=3.14 avg=1.42\n",
            "[7630 | 6693.05] loss=0.54 avg=1.42\n",
            "[7640 | 6701.73] loss=1.58 avg=1.42\n",
            "[7650 | 6710.41] loss=1.12 avg=1.41\n",
            "[7660 | 6719.13] loss=0.51 avg=1.41\n",
            "[7670 | 6727.82] loss=1.02 avg=1.40\n",
            "[7680 | 6736.52] loss=1.66 avg=1.40\n",
            "[7690 | 6745.22] loss=0.16 avg=1.39\n",
            "[7700 | 6753.95] loss=1.77 avg=1.40\n",
            "[7710 | 6762.66] loss=0.60 avg=1.39\n",
            "[7720 | 6771.35] loss=0.86 avg=1.38\n",
            "[7730 | 6780.05] loss=1.51 avg=1.38\n",
            "[7740 | 6788.74] loss=0.73 avg=1.38\n",
            "[7750 | 6797.44] loss=1.13 avg=1.37\n",
            "[7760 | 6806.14] loss=1.40 avg=1.37\n",
            "[7770 | 6814.83] loss=0.34 avg=1.36\n",
            "[7780 | 6823.50] loss=0.77 avg=1.36\n",
            "[7790 | 6832.19] loss=1.14 avg=1.36\n",
            "[7800 | 6840.88] loss=1.17 avg=1.35\n",
            "[7810 | 6849.58] loss=1.50 avg=1.36\n",
            "[7820 | 6858.28] loss=2.41 avg=1.37\n",
            "[7830 | 6867.01] loss=2.47 avg=1.38\n",
            "[7840 | 6875.71] loss=0.51 avg=1.37\n",
            "[7850 | 6884.41] loss=2.86 avg=1.38\n",
            "[7870 | 6901.81] loss=2.35 avg=1.39\n",
            "[7880 | 6910.50] loss=0.69 avg=1.38\n",
            "[7890 | 6919.17] loss=0.46 avg=1.37\n",
            "[7900 | 6927.86] loss=0.31 avg=1.36\n",
            "[7910 | 6936.54] loss=1.03 avg=1.36\n",
            "[7920 | 6945.22] loss=0.83 avg=1.35\n",
            "[7930 | 6953.91] loss=1.29 avg=1.35\n",
            "[7940 | 6962.60] loss=0.71 avg=1.35\n",
            "[7950 | 6971.30] loss=1.48 avg=1.35\n",
            "[7960 | 6980.00] loss=0.75 avg=1.34\n",
            "[7970 | 6988.68] loss=1.00 avg=1.34\n",
            "[7980 | 6997.37] loss=0.29 avg=1.33\n",
            "[7990 | 7006.08] loss=0.76 avg=1.32\n",
            "[8000 | 7014.77] loss=1.96 avg=1.33\n",
            "Saving checkpoint/run42K/model-8000\n",
            "[8010 | 7030.77] loss=0.21 avg=1.32\n",
            "[8020 | 7039.43] loss=1.95 avg=1.32\n",
            "[8030 | 7048.11] loss=1.05 avg=1.32\n",
            "[8040 | 7056.82] loss=0.67 avg=1.32\n",
            "[8050 | 7065.51] loss=1.85 avg=1.32\n",
            "[8060 | 7074.19] loss=1.74 avg=1.33\n",
            "[8070 | 7082.91] loss=0.40 avg=1.32\n",
            "[8080 | 7091.60] loss=0.89 avg=1.31\n",
            "[8090 | 7100.32] loss=0.34 avg=1.30\n",
            "[8100 | 7109.03] loss=1.36 avg=1.30\n",
            "[8110 | 7117.72] loss=0.87 avg=1.30\n",
            "[8120 | 7126.42] loss=0.98 avg=1.29\n",
            "[8130 | 7135.12] loss=0.28 avg=1.28\n",
            "[8140 | 7143.83] loss=2.29 avg=1.29\n",
            "[8150 | 7152.52] loss=0.77 avg=1.29\n",
            "[8160 | 7161.22] loss=1.27 avg=1.29\n",
            "[8170 | 7169.93] loss=1.89 avg=1.30\n",
            "[8180 | 7178.63] loss=0.35 avg=1.29\n",
            "[8190 | 7187.32] loss=0.65 avg=1.28\n",
            "[8200 | 7195.99] loss=1.60 avg=1.28\n",
            "[8210 | 7204.69] loss=1.42 avg=1.28\n",
            "[8220 | 7213.37] loss=0.89 avg=1.28\n",
            "[8230 | 7222.05] loss=0.94 avg=1.28\n",
            "[8240 | 7230.73] loss=0.93 avg=1.27\n",
            "[8250 | 7239.41] loss=0.45 avg=1.27\n",
            "[8260 | 7248.10] loss=1.18 avg=1.26\n",
            "[8270 | 7256.80] loss=1.34 avg=1.27\n",
            "[8280 | 7265.51] loss=2.27 avg=1.28\n",
            "[8290 | 7274.21] loss=1.31 avg=1.28\n",
            "[8300 | 7282.89] loss=0.57 avg=1.27\n",
            "[8310 | 7291.59] loss=1.37 avg=1.27\n",
            "[8320 | 7300.29] loss=0.21 avg=1.26\n",
            "[8330 | 7308.97] loss=2.31 avg=1.27\n",
            "[8340 | 7317.69] loss=0.58 avg=1.26\n",
            "[8350 | 7326.37] loss=2.30 avg=1.27\n",
            "[8360 | 7335.08] loss=0.82 avg=1.27\n",
            "[8370 | 7343.77] loss=1.09 avg=1.27\n",
            "[8380 | 7352.44] loss=0.94 avg=1.26\n",
            "[8390 | 7361.14] loss=0.99 avg=1.26\n",
            "[8400 | 7369.83] loss=1.27 avg=1.26\n",
            "[8410 | 7378.54] loss=1.59 avg=1.26\n",
            "[8420 | 7387.24] loss=2.09 avg=1.27\n",
            "[8430 | 7395.95] loss=0.39 avg=1.26\n",
            "[8440 | 7404.66] loss=1.06 avg=1.26\n",
            "[8450 | 7413.36] loss=0.56 avg=1.25\n",
            "[8460 | 7422.03] loss=3.32 avg=1.27\n",
            "[8470 | 7430.73] loss=0.67 avg=1.27\n",
            "[8480 | 7439.44] loss=1.41 avg=1.27\n",
            "[8490 | 7448.13] loss=0.77 avg=1.27\n",
            "[8500 | 7456.81] loss=0.21 avg=1.25\n",
            "[8510 | 7465.51] loss=2.11 avg=1.26\n",
            "[8520 | 7474.20] loss=1.06 avg=1.26\n",
            "[8530 | 7482.91] loss=1.25 avg=1.26\n",
            "[8540 | 7491.61] loss=0.71 avg=1.26\n",
            "[8550 | 7500.30] loss=0.98 avg=1.25\n",
            "[8560 | 7508.98] loss=2.84 avg=1.27\n",
            "[8570 | 7517.64] loss=2.81 avg=1.28\n",
            "[8580 | 7526.34] loss=2.19 avg=1.29\n",
            "[8590 | 7535.04] loss=2.18 avg=1.30\n",
            "[8600 | 7543.75] loss=0.68 avg=1.30\n",
            "[8610 | 7552.44] loss=0.64 avg=1.29\n",
            "[8620 | 7561.12] loss=0.32 avg=1.28\n",
            "[8630 | 7569.82] loss=1.38 avg=1.28\n",
            "[8640 | 7578.52] loss=0.42 avg=1.27\n",
            "[8650 | 7587.22] loss=0.75 avg=1.27\n",
            "[8660 | 7595.94] loss=0.23 avg=1.26\n",
            "[8670 | 7604.65] loss=0.41 avg=1.25\n",
            "[8680 | 7613.34] loss=0.16 avg=1.24\n",
            "[8690 | 7622.02] loss=1.22 avg=1.24\n",
            "[8700 | 7630.70] loss=1.89 avg=1.24\n",
            "[8710 | 7639.42] loss=0.26 avg=1.23\n",
            "[8720 | 7648.10] loss=2.20 avg=1.24\n",
            "[8730 | 7656.78] loss=0.53 avg=1.24\n",
            "[8740 | 7665.46] loss=0.64 avg=1.23\n",
            "[8750 | 7674.19] loss=0.63 avg=1.22\n",
            "[8760 | 7682.89] loss=0.39 avg=1.22\n",
            "[8770 | 7691.59] loss=0.34 avg=1.21\n",
            "[8780 | 7700.27] loss=0.75 avg=1.20\n",
            "[8790 | 7708.98] loss=1.77 avg=1.21\n",
            "[8800 | 7717.65] loss=0.89 avg=1.20\n",
            "[8810 | 7726.36] loss=0.21 avg=1.19\n",
            "[8820 | 7735.05] loss=1.08 avg=1.19\n",
            "[8830 | 7743.74] loss=1.26 avg=1.19\n",
            "[8840 | 7752.43] loss=1.49 avg=1.20\n",
            "[8850 | 7761.14] loss=0.44 avg=1.19\n",
            "[8860 | 7769.85] loss=2.02 avg=1.20\n",
            "[8870 | 7778.57] loss=1.50 avg=1.20\n",
            "[8880 | 7787.27] loss=1.24 avg=1.20\n",
            "[8890 | 7795.96] loss=0.64 avg=1.20\n",
            "[8900 | 7804.66] loss=0.43 avg=1.19\n",
            "[8910 | 7813.36] loss=0.72 avg=1.18\n",
            "[8920 | 7822.07] loss=2.19 avg=1.19\n",
            "[8930 | 7830.76] loss=2.16 avg=1.20\n",
            "[8940 | 7839.45] loss=1.64 avg=1.21\n",
            "[8950 | 7848.16] loss=0.51 avg=1.20\n",
            "[8960 | 7856.85] loss=0.77 avg=1.20\n",
            "[8970 | 7865.57] loss=0.35 avg=1.19\n",
            "[8980 | 7874.27] loss=1.39 avg=1.19\n",
            "[8990 | 7882.96] loss=1.16 avg=1.19\n",
            "[9000 | 7891.67] loss=0.78 avg=1.19\n",
            "Saving checkpoint/run42K/model-9000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VHdTL8NDbAh3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "gpt2.copy_checkpoint_to_gdrive(run_name='run42K')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pel-uBULXO2L",
        "colab_type": "text"
      },
      "source": [
        "## Load a Trained Model Checkpoint"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DCcx5u7sbPTD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "gpt2.copy_checkpoint_from_gdrive(run_name='run42K')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-fxL77nvAMAX",
        "colab_type": "code",
        "outputId": "dca9f00e-c1bd-4f8b-a448-769d6cab05df",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "sess = gpt2.start_tf_sess()\n",
        "gpt2.load_gpt2(sess, run_name='run42K')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loading checkpoint checkpoint/run42K/model-9000\n",
            "INFO:tensorflow:Restoring parameters from checkpoint/run42K/model-9000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ClJwpF_ACONp",
        "colab_type": "text"
      },
      "source": [
        "## Generate Text From The Trained Model\n",
        "\n",
        "Same as normal generate functions, except with additional parameters to handle the new tokens."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4RNY6RBI9LmL",
        "colab_type": "code",
        "outputId": "2cc1b26c-e0d2-4714-a3b4-bb1648bd6892",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "gpt2.generate(sess, run_name='run42K',\n",
        "             length=400,\n",
        "             prefix=\"<|startoftext|>[WP] You upload your brain to a computer and find a document that says the word 'God' in the title.  [RESPONSE]\",\n",
        "             truncate=\"<|endoftext|>\",\n",
        "             include_prefix=True)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<|startoftext|>[WP] You upload your brain to a computer and find a document that says the word 'God' in the title.  [RESPONSE] They did not want to have their own God, but it was that title which made them unhappy. \n",
            " \n",
            "I was born an ordinary man, with a simple goal: to love his neighbor as he loved himself.  That was the middle-class upbringing my parents wanted for me.  If I had been raised in a gladiator's camp, a brothel, or a slaver's den, God would have frowned upon me.  But I was a soldier, fighting the disease that was plaguing my home planet.  That was the war.  If I had not been a soldier, fighting the disease, I would have been nothing but an observer.  My talents did not earn me favor with the Supreme Ruler, but He saw a man with talent like mine, and He saw a man with skill like mine.  That is how the conversation went when I asked to be his student.  It was the seventh grade when I asked to be a scout, and the Supreme Ruler was pleased with my performance.  He told me that He favored me, and that He would train me.  I was grateful for His favor, but it saddened me that He had chosen to instruct me in His spiritual realm.  I was to use my talents to teach His creation, the humans.  To use technology to better understand the divine, and to spread His word to all who would listen.  \n",
            "\n",
            "It was these ideals that guided my career.  I was called a scientist by my human peers, but I was more than a science man.  I was a man, even a warrior.  I was a thinker, a beater, a shot-caller, and a slayer.  In the hands of a sage like myself, His teachings were knowledge for the greater good.  He wanted the humans to understand the greater good, so He created me to accomplish more than just that.  To change the world.  To do more than\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8DKMc0fiej4N",
        "colab_type": "code",
        "outputId": "7e7ebd2f-c4fc-4feb-f18c-4dd186adbeac",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "gpt2.generate(sess, run_name='run42K',\n",
        "              length=400,\n",
        "              temperature=.7,\n",
        "              nsamples=10,\n",
        "              batch_size=10,\n",
        "              prefix=\"<|startoftext|>[WP] Your alternative universe self has decided to tell you about the world that she built, and you're all her imagination. [RESPONSE]\",\n",
        "              truncate=\"<|endoftext|>\",\n",
        "              include_prefix=True\n",
        "              )"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<|startoftext|>[WP] Your alternative universe self has decided to tell you about the world that she built, and you're all her imagination. [RESPONSE]\n",
            "\n",
            "The book was heavy, when I first picked it up.\n",
            "\n",
            "It had to be heavy because it was a brand new, completely empty, book. That was a bit disappointing. It kind of reminded me of Harry Potter, which I had to say is probably a bit similar.\n",
            "\n",
            "I suppose it shouldn't matter because I just want to read it.\n",
            "\n",
            "I picked it up, opened it, and turned off the lights. I'm not sure why, but the book seemed to be implying that time had left me and that I was an immortal time traveler.\n",
            "\n",
            "And that's when I noticed that the pages were blank.\n",
            "\n",
            "I felt a bit of unease when I realized that I couldn't remember how I got here or what I was doing there. I was stuck in a loop, back in the library where I had gotten lost.\n",
            "\n",
            "But then, I noticed that the loop was getting longer by the second page.\n",
            "\n",
            "It was like I had been pulled into the loop, into the same physical place that I had gotten lost in. But I had no idea why.\n",
            "\n",
            "Then, I remembered.\n",
            "\n",
            "I looked at the book. It wasn't the \"Harry Potter\" series that was often cited, but there was a \"Dumbledore's Army\" book that had some similarities. Both were full of plot hooks, and I don't think I've read all of them.\n",
            "\n",
            "I looked up at the clock. It was sunday evening.\n",
            "\n",
            "It was already eleven o'clock.\n",
            "\n",
            "I sighed.\n",
            "\n",
            "I looked at the door. It was already eleven o'clock.\n",
            "\n",
            "I looked at the book. It was already eleven.\n",
            "\n",
            "I looked at the book again. It was already eleven.\n",
            "\n",
            "I looked at the book, and looked at the door.\n",
            "\n",
            "It was already eleven.\n",
            "\n",
            "I looked at the book. It was already eleven.\n",
            "\n",
            "\n",
            "====================\n",
            "<|startoftext|>[WP] Your alternative universe self has decided to tell you about the world that she built, and you're all her imagination. [RESPONSE]\n",
            "\n",
            "The line went dead in my throat. It wasn't the first or last time that it would go unspoken. After the second and most powerful of the spoken challenges, it took on a special meaning to me, as if the reason I was here was to be repeated.\n",
            "\n",
            "I don't know how it was made here. I don't know why I was brought here. I don't know why this place was made. All I know is that I am here.\n",
            "\n",
            "I don't know why I came to this place, but I do know that this place was made for me. This place was made for me. This place is where my power and my duty as a goddess lay. It was designed for me. This place is where I belong.\n",
            "\n",
            "I belong here.\n",
            "\n",
            "WETTER THAN ANY OTHER WETCHES I HAVE EVER FOUND, this place is set up to be explored. It is where I fit my duty. I belong here.\n",
            "\n",
            "WATER FLIGHTS OPPORTUNITY - I belong here.\n",
            "\n",
            "A sudden, violent howl erupted from the depths of the well. Up ahead, the source of the howl was revealed, a massive, alien vessel that seemed to stand apart in its tracks. It didn't matter how far out the ship I came in, the vastness of it was greater than anything I'd ever imagined.\n",
            "\n",
            "And yet, the source of it all was the same: a single spherical engine with no markings. It was unmistakable. The same one that had been seen in the photos of the source. The one that had been found in the ocean, thousands of years ago.\n",
            "\n",
            "This place, this is where my story begins.\n",
            "====================\n",
            "<|startoftext|>[WP] Your alternative universe self has decided to tell you about the world that she built, and you're all her imagination. [RESPONSE]\n",
            "\n",
            "I built the most egotistical universe the world has ever seen.\n",
            "\n",
            "My power wasn't going to make or break me. It wasn't going to create a utopia or a dystopia, or really *anything* at all. After a while I realized that I was just a *supplemental* being in a \"supplement\" being, and that the universe was only ever going to create a single consciousness in the people who called themselves \"humans\" - that is to say, until it became apparent to the people on the other side that they were just getting their minds blown.\n",
            "\n",
            "I became a god.\n",
            "\n",
            "I built four cities - two more should be coming - and ruled over a half a dozen species. I built hospitals, schools, made houses, and generally built *more* human-friendly universes. I built entire worlds which people could be proud of, and made them into happy, fulfilled lives.\n",
            "\n",
            "I sat on the throne of all the worlds I built and enjoyed the peace and contentment of them all.\n",
            "\n",
            "I had a son, who I raised as a god, and whom I called \"Skippy\" because he was so happy being around humans. He became a god after his father, and was a god for a very good reason.\n",
            "\n",
            "I was never happier than when I was around him.\n",
            "\n",
            "I saw him grow from a crow to a crow-like creature, and felt sadness set into his heart as he pondered my actions. It was the first time I had ever seen any of his emotions, and he chose the one that brought him so much pain. He couldn't have felt anything had been the opposite of what I did.\n",
            "\n",
            "He said nothing to me, and only said those things that came out of his mouth.\n",
            "\n",
            "I wiped the tears from his face with the back of one arm, and decided I would stay by his side through this dark time.\n",
            "\n",
            "\n",
            "====================\n",
            "<|startoftext|>[WP] Your alternative universe self has decided to tell you about the world that she built, and you're all her imagination. [RESPONSE]\n",
            "\n",
            "\"You have a choice here, girl.\"\n",
            "\n",
            "I grinned and nudged the button. The dial tone was a cheery voice, like that of the woman I knew, and all the signs indicated that this was a woman I could trust.\n",
            "\n",
            "\"It's just a game. But if you don't, then the consequences are certain. I'm building a world with you, earth. Together.\"\n",
            "\n",
            "I felt my stomach flip-flop. It had been so long since I'd felt human, back when she'd first come to me. But somehow, she had convinced me to build her a world where I could live. I was aghast, even with my eyes open. She could build me a world with who-the-fuck-I-just-saw-last-week.\n",
            "\n",
            "She opened the door, and I followed her. The walls were a lattice of flesh, with lines of blood gushing forth from every millimeter of our acquaintance. She sighed. \"Sweet baby Jesus.\"\n",
            "\n",
            "\"Who are you?\" I asked, even as my conscience protested.\n",
            "\n",
            "\"I'm your father. Don't be. I'm here so you know that what you're experiencing is real.\"\n",
            "\n",
            "My father had always been a good man. Even when my mother was murdered by a drunk driver during a robbery at gunpoint two years ago, he had remained on the side of the road helping the victims.\n",
            "\n",
            "And he was fine. I didn't ask for my father's memory, but he was fine. Sorry to bother you, but he was always fine.\n",
            "\n",
            "And now I had the chance to build a new life with my own father. For me.\n",
            "\n",
            "\"You're too young for a game like this,\" she said, ushering me into a large waiting room. \"But you'll have a chance to make a better one.\"\n",
            "\n",
            "I was raised by my single mother.\n",
            "====================\n",
            "<|startoftext|>[WP] Your alternative universe self has decided to tell you about the world that she built, and you're all her imagination. [RESPONSE]\n",
            "\n",
            "Unless I remember the ending...\n",
            "\n",
            "The door to Dr. Hall's office opened and the young intern stepped into the room. She'd been on the job for less than two hours and was tired, but her work always seemed to fill her life. She sat down in Dr. Hall's study one Monday morning and found the ethical quandary in her job: how could you know what was best for humanity if you didn't know the reasons?\n",
            "\n",
            "\"Self-aware?\" she asked. \"I don't think there's a word for 'Sci-Fi' in this room.\"\n",
            "\n",
            "Dr. Hall's head tilted in his chair. He scanned the intern's shoulder for any movement, but Allison didn't move. She'd been steadily walking from the chair all day.\n",
            "\n",
            "\"Self-aware?\" She repeated. \"In a sense, I suppose you could say that this is what we are: a collection of individual nuggets of information that have been extracted from the singularity that brought us all into being. However, this is not the case. This is the explanation that best explains the origins of the Collective that we are now.\"\n",
            "\n",
            "Dr. Hall nodded. The scientists below him nodded in understanding.\n",
            "\n",
            "\"The Collective is not a singularity. It is rather a set of rules and regulations that binds together a being in its entirety. Rules and regulations that make it nearly impossible to independently conceive, retain, or regain. As a result, we exist in a state of perpetual vigilance.\"\n",
            "\n",
            "Dr. Hall paused to take in the situation. Allison was standing behind him, her posture slumping slightly as she averted her gaze.\n",
            "\n",
            "\"In other words,\" Dr. Hall continued, \"if we are correct in our theory, then what we are experiencing is indeed the Collective's fault. However, what we can do is to design a new protocol that does not require the Collective to create a new singularity in our\n",
            "====================\n",
            "<|startoftext|>[WP] Your alternative universe self has decided to tell you about the world that she built, and you're all her imagination. [RESPONSE]\n",
            "\n",
            "I built her the life I wanted. From the moment my brow was pressed against the keys as she typed, I knew that this was where my story would begin.\n",
            "\n",
            "I stared at the keyboard. I knew this was where my story would begin.\n",
            "\n",
            "I heard the clicking of keys that were meant to answer my questions, but instead, I heard her.\n",
            "\n",
            "“Hello, I’m Jane Doe. I’m your editor. What’s going on?”\n",
            "\n",
            "The moment my brow was lowered, I felt an incredible loneliness. I felt like I could hear my entire world, playing on repeat in my head.\n",
            "\n",
            "“Where did you get your idea for a blank slate?”\n",
            "\n",
            "“It came from that book,” she said, tapping the cover of the book that had been buried at the bottom of my closet for so long. “It had an original manuscript, but the formatting was never right.”\n",
            "\n",
            "She slid the book across the table. “This is the story of Dr. Ruthven. Dr. Ruthven. How did you find out about this?”\n",
            "\n",
            "“From that book,” she said, putting the book back in the box. “It had an address on the inside, but the book never claimed to have an address in this world. Only a scribe could have opened it. And they all knew each other.”\n",
            "\n",
            "I sat back in my chair. “Wow.”\n",
            "\n",
            "“Now, come out. I’m ready to go to my first class.”\n",
            "\n",
            "I glanced at my empty backpack. “I’m still going to my first date tonight.”\n",
            "\n",
            "WritingPrompts\n",
            "The_Windwalker\n",
            "1 points\n",
            "2 months ago\n",
            "I was so happy to find this place. I usually would have\n",
            "====================\n",
            "<|startoftext|>[WP] Your alternative universe self has decided to tell you about the world that she built, and you're all her imagination. [RESPONSE]\n",
            "\n",
            "So, she's not really a robot, she's just a part of my own that's been jettisoned to make way for the new world I build.\n",
            "\n",
            "I'm the undisputed ruler of this new world, and no one can top me here.\n",
            "\n",
            "I'm the god of this new world, and everyone here is my god.\n",
            "\n",
            "Everyone here is just...hmmph\n",
            "\n",
            "Looking at the time, it's been about 30 years since anyone was alive in this world.\n",
            "\n",
            "So, yeah, I guess I'm the undisputed ruler of this new world.\n",
            "\n",
            "I also happen to be the only person that knows how to make her own coffee.\n",
            "\n",
            "I'm the sole owner of the world’s most famous knife.\n",
            "\n",
            "So, let’s make a toast, shall we?\n",
            "\n",
            "WritingPrompts\n",
            "Tramelo\n",
            "1 points\n",
            "6 months ago\n",
            "\"Urgent message from the White House,\" announced the voice over the intercom. \"The President is not amused.\"\n",
            "\n",
            "\"Who are you?\" asked the President.\n",
            "\n",
            "\"I’m the only one that knows how to make my own coffee, the other members of the family are still figuring out what happens.\"\n",
            "\n",
            "\"Your coffee?\"\n",
            "\n",
            "\"The one in the corner, the one that's not ready yet.\"\n",
            "\n",
            "\"Is that okay?\"\n",
            "\n",
            "\"I don’t think so. Are you sure it isn’t going to end up being a rioting incident?\"\n",
            "\n",
            "\"It won’t be a riot, though.\" The President shook his head. \"I’m just going to ask you to stay in your seat, it won’t affect the quality of your coffee.\"\n",
            "\n",
            "\"You are going to tell me what the heck is going on?\"\n",
            "\n",
            "\"I need to know, okay? If you want to know,\n",
            "====================\n",
            "<|startoftext|>[WP] Your alternative universe self has decided to tell you about the world that she built, and you're all her imagination. [RESPONSE]\n",
            "\n",
            "The first night I built this place, I was ecstatic. It was my first creative endeavor, and the reward was just immeasurable. The possibilities were endless, immeasurable as the universe, and I was the first to find them.\n",
            "\n",
            "The second night, I was even more ecstatic. This was my chance to prove once and for all that I could build things in my own right out of nothing, to create things that others would be proud to call their own.\n",
            "\n",
            "The third night, came with a bang. The fourth. I was ready. I could see the universe as my tiny sandbox, and I was ready to build a world, to share it with others.\n",
            "\n",
            "But first, I had to get it built.\n",
            "\n",
            "The first brick was cast relatively quickly, and the next two took only a few minutes each. There was a tense moment waiting at the end of the hallway when I decided to connect the pieces together. I had to get the dimensions right, the lines needed to cross, and then it could go into the building.\n",
            "\n",
            "I was very pleased with the end results, but I knew I needed more space. So much space that I had to block the doorway completely at this point.\n",
            "\n",
            "The walls of this place were wonderfully detailed, and I marveled at how the author managed to use various techniques to depict various spaces that were both extremely cramped and also extremely wide. There was a section just over a mile in length where the author described how the space got so big that it swallowed up the entire town. Amazing stuff!\n",
            "\n",
            "After the space was filled, I was able to get to my work. It was a huge, hall of mirrors that stretched for everything. I could make various descriptions of the spaces that I needed space for. For example, a room that swallowed up a town. A library that fed millions in, just like a public library.\n",
            "\n",
            "I had a great time describing\n",
            "====================\n",
            "<|startoftext|>[WP] Your alternative universe self has decided to tell you about the world that she built, and you're all her imagination. [RESPONSE]\n",
            "\n",
            "I thought I was living in a dream.\n",
            "\n",
            "There was no need to think about it. I was simply imagining things. Things that were out of the ordinary, things that didn’t make sense. Things that were different for the reasons stated above.\n",
            "\n",
            "I was wrong.\n",
            "\n",
            "I was waking up in a body I had never known. I was having a mental election. I couldn’t remember what I was elected for, but I knew that I was elected to a task that I couldn’t even begin to imagine.\n",
            "\n",
            "I remember only one other thing. That was the day I was walking in the hallway and saw my best friend’s body laying on the floor, there was no mark nor any trace of foul play. I thought nothing of it.\n",
            "\n",
            "Then, the day I realized that it was the body of my childhood best friend, Fred.\n",
            "\n",
            "Fred died the following day from complications of AIDS. By that point, I had spent the rest of my life in a constant state of medication. I had tried various treatments, from going under the sea in a cruise ship, to much simpler and less invasive procedures, like puncturing the veins of the baby I had with a rock.\n",
            "\n",
            "I had tried various remedies that were essentially forbidding, like forbidding bacteria to grow in the womb.\n",
            "\n",
            "I’d tried freezing Fred, hoping that he would just pass along the virus to his newborn child.\n",
            "\n",
            "I had tried inserting wooden rods into his stomach, hoping that he would be mentally coerced into becoming a human tube of lubricant.\n",
            "\n",
            "And now, I was standing in a body I’d never known, staring at a blank chest.\n",
            "\n",
            "“How did you do that?” I asked.\n",
            "\n",
            "I was too young to have ever seen a body before, so I could not fully know how the mind worked. I also could not fully know\n",
            "====================\n",
            "<|startoftext|>[WP] Your alternative universe self has decided to tell you about the world that she built, and you're all her imagination. [RESPONSE]\n",
            "\n",
            "\"Hey, what's up?\"\n",
            "\n",
            "\"Hey, what's up?\"\n",
            "\n",
            "\"I... I just wanted to know.\"\n",
            "\n",
            "\"Yeah, I get it.\"\n",
            "\n",
            "\"I was thinking,\" He chuckled, \"about trying to build a little empire around your very universe?\"\n",
            "\n",
            "\"I guess I can't complain about that, huh?\"\n",
            "\n",
            "\"Try building an empire around the worlds of the people that you make fun of.\"\n",
            "\n",
            "\"That's a good thing, I guess?\"\n",
            "\n",
            "\"Yeah. I mean, I can't complain about that either; it means I get to keep spaying and hare-phobic and all that.\"\n",
            "\n",
            "\"Oh, yeah. You can't. You can't have millions of people talking about your universe. That'd just be boring.\"\n",
            "\n",
            "\"Stop me when you get bored.\"\n",
            "\n",
            "\"Babe? What's up?\"\n",
            "\n",
            "\"Nothing important.\"\n",
            "\n",
            "\"Oh... BOOOOOOM!\"\n",
            "\n",
            "-\n",
            "\n",
            "The rest was a little sad. Like, really sad. Like, there was nothing to celebrate. Nothing to show love for. Nothing to try to make my life better.\n",
            "\n",
            "The main event, however, was what happened the next day.\n",
            "\n",
            "Monday was the day I told her I wanted to be alone.\n",
            "\n",
            "I was sad because I thought she would be sad the whole afternoon. I was sad because I wanted to spend my whole life with her. I was sad because I wanted to be able to go back to being me. And I was sad because I didn't want to be alone anymore.\n",
            "\n",
            "I told her that I was sorry. I was ready to go back to being me. I was not sad because I didn't want to be alone anymore.\n",
            "\n",
            "And then, I told her, I told her many times, but she would not listen.\n",
            "\n",
            "I told her about my life, my\n",
            "====================\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mP7iHAhCAEKo",
        "colab_type": "text"
      },
      "source": [
        "If generating in bulk, you may want to set `sample_demin=''` to remove the delimiter between each sample."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fa6p6arifSL0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "gen_file = 'gpt2_gentext_{:%Y%m%d_%H%M%S}.txt'.format(datetime.utcnow())\n",
        "\n",
        "gpt2.generate_to_file(sess,\n",
        "                      destination_path=gen_file,\n",
        "                      length=100,\n",
        "                      temperature=1.0,\n",
        "                      nsamples=100,\n",
        "                      batch_size=20,\n",
        "                      prefix=\"<|startoftext|>\",\n",
        "                      truncate=\"<|endoftext|>\",\n",
        "                      include_prefix=False,\n",
        "                      sample_delim=''\n",
        "                      )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0-LRex8lfv1g",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# may have to run twice to get file to download\n",
        "files.download(gen_file)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ig-KVgkCDCKD",
        "colab_type": "text"
      },
      "source": [
        "# Etcetera\n",
        "\n",
        "If the notebook has errors (e.g. GPU Sync Fail), force-kill the Colaboratory virtual machine and restart it with the command below:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rIHiVP53FnsX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!kill -9 -1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wmTXWNUygS5E",
        "colab_type": "text"
      },
      "source": [
        "# LICENSE\n",
        "\n",
        "MIT License\n",
        "\n",
        "Copyright (c) 2019 Max Woolf\n",
        "\n",
        "Permission is hereby granted, free of charge, to any person obtaining a copy\n",
        "of this software and associated documentation files (the \"Software\"), to deal\n",
        "in the Software without restriction, including without limitation the rights\n",
        "to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n",
        "copies of the Software, and to permit persons to whom the Software is\n",
        "furnished to do so, subject to the following conditions:\n",
        "\n",
        "The above copyright notice and this permission notice shall be included in all\n",
        "copies or substantial portions of the Software.\n",
        "\n",
        "THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n",
        "IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n",
        "FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n",
        "AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n",
        "LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n",
        "OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n",
        "SOFTWARE."
      ]
    }
  ]
}