{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Writing Prompts Prompter",
      "provenance": [],
      "collapsed_sections": [
        "BT__brhBCvJu",
        "LdpZQXknFNY3"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H7LoMj4GA4n_",
        "colab_type": "text"
      },
      "source": [
        "#  Finetune GPT-2 on Reddit Data\n",
        "\n",
        "by [Max Woolf](http://minimaxir.com)\n",
        "\n",
        "A variant of the [default notebook](https://colab.research.google.com/drive/1VLG8e7YSEwypxU-noRNhsv5dW4NfTGce) optimized for short-form titles. It is recommended to be familiar with that notebook before using this one.\n",
        "\n",
        "This example uses a CSV export of Reddit data via BigQuery (see this post for more information).\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KBkpRgBCBS2_",
        "colab_type": "code",
        "outputId": "d501a90f-9e7a-4648-c6df-b6b98d1a8d9a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 233
        }
      },
      "source": [
        "!pip install -q gpt-2-simple\n",
        "import gpt_2_simple as gpt2\n",
        "from datetime import datetime\n",
        "from google.colab import files"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[?25l\r\u001b[K     |▌                               | 10kB 21.0MB/s eta 0:00:01\r\u001b[K     |█                               | 20kB 26.1MB/s eta 0:00:01\r\u001b[K     |█▌                              | 30kB 6.5MB/s eta 0:00:01\r\u001b[K     |██                              | 40kB 8.2MB/s eta 0:00:01\r\u001b[K     |██▌                             | 51kB 9.8MB/s eta 0:00:01\r\u001b[K     |███                             | 61kB 11.2MB/s eta 0:00:01\r\u001b[K     |███▋                            | 71kB 12.5MB/s eta 0:00:01\r\u001b[K     |████                            | 81kB 13.7MB/s eta 0:00:01\r\u001b[K     |████▋                           | 92kB 8.9MB/s eta 0:00:01\r\u001b[K     |█████                           | 102kB 9.7MB/s eta 0:00:01\r\u001b[K     |█████▋                          | 112kB 9.7MB/s eta 0:00:01\r\u001b[K     |██████                          | 122kB 9.7MB/s eta 0:00:01\r\u001b[K     |██████▋                         | 133kB 9.7MB/s eta 0:00:01\r\u001b[K     |███████▏                        | 143kB 9.7MB/s eta 0:00:01\r\u001b[K     |███████▋                        | 153kB 9.7MB/s eta 0:00:01\r\u001b[K     |████████▏                       | 163kB 9.7MB/s eta 0:00:01\r\u001b[K     |████████▋                       | 174kB 9.7MB/s eta 0:00:01\r\u001b[K     |█████████▏                      | 184kB 9.7MB/s eta 0:00:01\r\u001b[K     |█████████▊                      | 194kB 9.7MB/s eta 0:00:01\r\u001b[K     |██████████▏                     | 204kB 9.7MB/s eta 0:00:01\r\u001b[K     |██████████▊                     | 215kB 9.7MB/s eta 0:00:01\r\u001b[K     |███████████▏                    | 225kB 9.7MB/s eta 0:00:01\r\u001b[K     |███████████▊                    | 235kB 9.7MB/s eta 0:00:01\r\u001b[K     |████████████▏                   | 245kB 9.7MB/s eta 0:00:01\r\u001b[K     |████████████▊                   | 256kB 9.7MB/s eta 0:00:01\r\u001b[K     |█████████████▎                  | 266kB 9.7MB/s eta 0:00:01\r\u001b[K     |█████████████▊                  | 276kB 9.7MB/s eta 0:00:01\r\u001b[K     |██████████████▎                 | 286kB 9.7MB/s eta 0:00:01\r\u001b[K     |██████████████▊                 | 296kB 9.7MB/s eta 0:00:01\r\u001b[K     |███████████████▎                | 307kB 9.7MB/s eta 0:00:01\r\u001b[K     |███████████████▉                | 317kB 9.7MB/s eta 0:00:01\r\u001b[K     |████████████████▎               | 327kB 9.7MB/s eta 0:00:01\r\u001b[K     |████████████████▉               | 337kB 9.7MB/s eta 0:00:01\r\u001b[K     |█████████████████▎              | 348kB 9.7MB/s eta 0:00:01\r\u001b[K     |█████████████████▉              | 358kB 9.7MB/s eta 0:00:01\r\u001b[K     |██████████████████▎             | 368kB 9.7MB/s eta 0:00:01\r\u001b[K     |██████████████████▉             | 378kB 9.7MB/s eta 0:00:01\r\u001b[K     |███████████████████▍            | 389kB 9.7MB/s eta 0:00:01\r\u001b[K     |███████████████████▉            | 399kB 9.7MB/s eta 0:00:01\r\u001b[K     |████████████████████▍           | 409kB 9.7MB/s eta 0:00:01\r\u001b[K     |████████████████████▉           | 419kB 9.7MB/s eta 0:00:01\r\u001b[K     |█████████████████████▍          | 430kB 9.7MB/s eta 0:00:01\r\u001b[K     |██████████████████████          | 440kB 9.7MB/s eta 0:00:01\r\u001b[K     |██████████████████████▍         | 450kB 9.7MB/s eta 0:00:01\r\u001b[K     |███████████████████████         | 460kB 9.7MB/s eta 0:00:01\r\u001b[K     |███████████████████████▍        | 471kB 9.7MB/s eta 0:00:01\r\u001b[K     |████████████████████████        | 481kB 9.7MB/s eta 0:00:01\r\u001b[K     |████████████████████████▍       | 491kB 9.7MB/s eta 0:00:01\r\u001b[K     |█████████████████████████       | 501kB 9.7MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▌      | 512kB 9.7MB/s eta 0:00:01\r\u001b[K     |██████████████████████████      | 522kB 9.7MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▌     | 532kB 9.7MB/s eta 0:00:01\r\u001b[K     |███████████████████████████     | 542kB 9.7MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▌    | 552kB 9.7MB/s eta 0:00:01\r\u001b[K     |████████████████████████████    | 563kB 9.7MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▌   | 573kB 9.7MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████   | 583kB 9.7MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▌  | 593kB 9.7MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 604kB 9.7MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▌ | 614kB 9.7MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████ | 624kB 9.7MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▋| 634kB 9.7MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 645kB 9.7MB/s \n",
            "\u001b[?25h  Building wheel for gpt-2-simple (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:\n",
            "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
            "For more information, please see:\n",
            "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
            "  * https://github.com/tensorflow/addons\n",
            "  * https://github.com/tensorflow/io (for I/O related ops)\n",
            "If you depend on functionality not listed there, please file an issue.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bj2IJLHP3KwE",
        "colab_type": "text"
      },
      "source": [
        "## GPU"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sUmTooTW3osf",
        "colab_type": "code",
        "outputId": "a09455db-b5d4-49a3-ca1e-a6715f69eafc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 343
        }
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Thu Nov 14 14:56:19 2019       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 430.50       Driver Version: 418.67       CUDA Version: 10.1     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla K80           Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   49C    P8    29W / 149W |      0MiB / 11441MiB |      0%      Default |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                       GPU Memory |\n",
            "|  GPU       PID   Type   Process name                             Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0wXB05bPDYxS",
        "colab_type": "text"
      },
      "source": [
        "## Downloading GPT-2\n",
        "\n",
        "The default query returns 1.3MB of data, so probably should only use `124M` GPT-2 to finetune. If working with more Reddity data, then migrate to `355M`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P8wSlgXoDPCR",
        "colab_type": "code",
        "outputId": "6425a306-f0eb-4a55-9390-47d29d91d95a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        }
      },
      "source": [
        "gpt2.download_gpt2(model_name=\"355M\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Fetching checkpoint: 1.05Mit [00:00, 551Mit/s]                                                      \n",
            "Fetching encoder.json: 1.05Mit [00:00, 106Mit/s]                                                    \n",
            "Fetching hparams.json: 1.05Mit [00:00, 356Mit/s]                                                    \n",
            "Fetching model.ckpt.data-00000-of-00001: 1.42Git [00:11, 122Mit/s]                                  \n",
            "Fetching model.ckpt.index: 1.05Mit [00:00, 367Mit/s]                                                \n",
            "Fetching model.ckpt.meta: 1.05Mit [00:00, 108Mit/s]                                                 \n",
            "Fetching vocab.bpe: 1.05Mit [00:00, 227Mit/s]                                                       \n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N8KXuKWzQSsN",
        "colab_type": "text"
      },
      "source": [
        "## Mounting Google Drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "puq4iC6vUAHc",
        "colab_type": "code",
        "outputId": "978fe5c7-6169-45de-d155-dc4638b3623a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "gpt2.mount_gdrive()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BT__brhBCvJu",
        "colab_type": "text"
      },
      "source": [
        "## Uploading a Text File to be Trained to Colaboratory\n",
        "\n",
        "A single-column CSV is expected."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6OFnPCLADfll",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "file_name = \"WritingPrompts50KPlusAI 10-23-2019.csv\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HeeSKtNWUedE",
        "colab_type": "text"
      },
      "source": [
        "If your text file is larger than 10MB, it is recommended to upload that file to Google Drive first, then copy that file from Google Drive to the Colaboratory VM."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Z6okFD8VKtS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "gpt2.copy_file_from_gdrive(file_name)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LdpZQXknFNY3",
        "colab_type": "text"
      },
      "source": [
        "## Finetune GPT-2\n",
        "\n",
        "Providing a single-column CSV will automatically add `<|startoftext|>` and `<|endoftext|>` tokens appropriately.\n",
        "\n",
        "Short form text is more likely to overfit, so train it with fewer steps than you would for longform content."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aeXshJM-Cuaf",
        "colab_type": "code",
        "outputId": "c3abc15a-16b5-4fc3-f1d9-1855b24e1603",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "sess = gpt2.start_tf_sess()\n",
        "\n",
        "gpt2.finetune(sess,\n",
        "              dataset=file_name,\n",
        "              model_name='355M',\n",
        "              steps=5000,\n",
        "              restore_from='latest',\n",
        "              run_name='run42K',\n",
        "              print_every=10,\n",
        "              sample_every=5000,\n",
        "              )"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/gpt_2_simple/src/sample.py:17: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/gpt_2_simple/src/memory_saving_gradients.py:62: get_backward_walk_ops (from tensorflow.contrib.graph_editor.select) is deprecated and will be removed after 2019-06-06.\n",
            "Instructions for updating:\n",
            "Please use tensorflow.python.ops.op_selector.get_backward_walk_ops.\n",
            "Loading checkpoint models/355M/model.ckpt\n",
            "INFO:tensorflow:Restoring parameters from models/355M/model.ckpt\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 1/1 [00:00<00:00, 12.09it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Loading dataset...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "dataset has 1065179 tokens\n",
            "Training...\n",
            "[10 | 44.49] loss=2.14 avg=2.14\n",
            "[20 | 75.63] loss=2.30 avg=2.22\n",
            "[30 | 106.76] loss=2.49 avg=2.31\n",
            "[40 | 137.89] loss=2.39 avg=2.33\n",
            "[50 | 169.04] loss=2.65 avg=2.40\n",
            "[60 | 200.18] loss=2.38 avg=2.39\n",
            "[70 | 231.34] loss=2.45 avg=2.40\n",
            "[80 | 262.41] loss=2.32 avg=2.39\n",
            "[90 | 293.48] loss=2.37 avg=2.39\n",
            "[100 | 324.54] loss=2.36 avg=2.39\n",
            "[110 | 355.62] loss=2.57 avg=2.40\n",
            "[120 | 386.69] loss=2.00 avg=2.37\n",
            "[130 | 417.76] loss=2.23 avg=2.36\n",
            "[140 | 448.87] loss=2.49 avg=2.37\n",
            "[150 | 479.96] loss=2.12 avg=2.35\n",
            "[160 | 511.05] loss=2.13 avg=2.33\n",
            "[170 | 542.13] loss=2.65 avg=2.35\n",
            "[180 | 573.24] loss=2.20 avg=2.35\n",
            "[190 | 604.35] loss=2.30 avg=2.34\n",
            "[200 | 635.51] loss=2.32 avg=2.34\n",
            "[210 | 666.55] loss=2.37 avg=2.34\n",
            "[220 | 697.67] loss=2.55 avg=2.35\n",
            "[230 | 728.77] loss=2.37 avg=2.35\n",
            "[240 | 759.89] loss=2.63 avg=2.37\n",
            "[250 | 791.04] loss=2.40 avg=2.37\n",
            "[260 | 822.09] loss=2.16 avg=2.36\n",
            "[270 | 853.19] loss=2.25 avg=2.35\n",
            "[280 | 884.38] loss=2.31 avg=2.35\n",
            "[290 | 915.55] loss=2.35 avg=2.35\n",
            "[300 | 946.62] loss=2.08 avg=2.34\n",
            "[310 | 977.68] loss=1.61 avg=2.32\n",
            "[320 | 1008.69] loss=1.98 avg=2.30\n",
            "[330 | 1039.78] loss=2.21 avg=2.30\n",
            "[340 | 1070.83] loss=2.19 avg=2.30\n",
            "[350 | 1101.92] loss=2.00 avg=2.29\n",
            "[360 | 1133.01] loss=1.93 avg=2.27\n",
            "[370 | 1164.05] loss=1.80 avg=2.26\n",
            "[380 | 1195.13] loss=2.19 avg=2.26\n",
            "[390 | 1226.20] loss=1.91 avg=2.25\n",
            "[400 | 1257.22] loss=1.85 avg=2.23\n",
            "[410 | 1288.25] loss=2.31 avg=2.24\n",
            "[420 | 1319.35] loss=2.22 avg=2.24\n",
            "[430 | 1350.45] loss=2.07 avg=2.23\n",
            "[440 | 1381.57] loss=1.52 avg=2.21\n",
            "[450 | 1412.70] loss=2.34 avg=2.22\n",
            "[460 | 1443.73] loss=2.01 avg=2.21\n",
            "[470 | 1474.81] loss=2.32 avg=2.21\n",
            "[480 | 1505.95] loss=2.06 avg=2.21\n",
            "[490 | 1537.11] loss=1.85 avg=2.20\n",
            "[500 | 1568.27] loss=2.42 avg=2.21\n",
            "[510 | 1599.37] loss=2.01 avg=2.20\n",
            "[520 | 1630.54] loss=1.53 avg=2.18\n",
            "[530 | 1661.70] loss=2.28 avg=2.19\n",
            "[540 | 1692.85] loss=2.45 avg=2.19\n",
            "[550 | 1723.91] loss=2.69 avg=2.20\n",
            "[560 | 1754.99] loss=1.85 avg=2.20\n",
            "[570 | 1786.04] loss=2.36 avg=2.20\n",
            "[580 | 1817.14] loss=2.11 avg=2.20\n",
            "[590 | 1848.21] loss=1.80 avg=2.19\n",
            "[600 | 1879.33] loss=2.41 avg=2.19\n",
            "[610 | 1910.40] loss=1.94 avg=2.19\n",
            "[620 | 1941.52] loss=1.96 avg=2.18\n",
            "[630 | 1972.67] loss=2.43 avg=2.19\n",
            "[640 | 2003.76] loss=1.72 avg=2.18\n",
            "[650 | 2034.90] loss=1.53 avg=2.17\n",
            "[660 | 2066.09] loss=2.11 avg=2.16\n",
            "[670 | 2097.14] loss=2.02 avg=2.16\n",
            "[680 | 2128.30] loss=2.27 avg=2.16\n",
            "[690 | 2159.40] loss=1.96 avg=2.16\n",
            "[700 | 2190.51] loss=1.60 avg=2.15\n",
            "[710 | 2221.55] loss=1.97 avg=2.14\n",
            "[720 | 2252.58] loss=2.40 avg=2.15\n",
            "[730 | 2283.65] loss=2.24 avg=2.15\n",
            "[740 | 2314.66] loss=1.78 avg=2.14\n",
            "[750 | 2345.66] loss=2.02 avg=2.14\n",
            "[760 | 2376.65] loss=1.22 avg=2.12\n",
            "[770 | 2407.68] loss=2.17 avg=2.13\n",
            "[780 | 2438.72] loss=1.90 avg=2.12\n",
            "[790 | 2469.72] loss=1.67 avg=2.11\n",
            "[800 | 2500.75] loss=1.99 avg=2.11\n",
            "[810 | 2531.83] loss=2.14 avg=2.11\n",
            "[820 | 2562.91] loss=2.16 avg=2.11\n",
            "[830 | 2594.00] loss=2.18 avg=2.11\n",
            "[840 | 2625.03] loss=1.64 avg=2.10\n",
            "[850 | 2656.04] loss=2.47 avg=2.11\n",
            "[860 | 2687.10] loss=1.43 avg=2.10\n",
            "[870 | 2718.18] loss=1.67 avg=2.09\n",
            "[880 | 2749.19] loss=2.28 avg=2.10\n",
            "[890 | 2780.22] loss=1.68 avg=2.09\n",
            "[900 | 2811.20] loss=2.29 avg=2.09\n",
            "[910 | 2842.28] loss=2.37 avg=2.10\n",
            "[920 | 2873.37] loss=1.69 avg=2.09\n",
            "[930 | 2904.44] loss=2.43 avg=2.10\n",
            "[940 | 2935.53] loss=2.02 avg=2.09\n",
            "[950 | 2966.65] loss=2.17 avg=2.10\n",
            "[960 | 2997.68] loss=2.30 avg=2.10\n",
            "[970 | 3028.75] loss=1.59 avg=2.09\n",
            "[980 | 3059.91] loss=1.44 avg=2.08\n",
            "[990 | 3091.10] loss=2.08 avg=2.08\n",
            "[1000 | 3122.15] loss=1.35 avg=2.07\n",
            "Saving checkpoint/run42K/model-1000\n",
            "[1010 | 3162.04] loss=1.99 avg=2.07\n",
            "[1020 | 3193.12] loss=2.03 avg=2.07\n",
            "[1030 | 3224.13] loss=2.14 avg=2.07\n",
            "[1040 | 3255.27] loss=1.43 avg=2.06\n",
            "[1050 | 3286.43] loss=1.46 avg=2.05\n",
            "[1060 | 3317.61] loss=2.50 avg=2.06\n",
            "[1070 | 3348.75] loss=1.57 avg=2.05\n",
            "[1080 | 3379.90] loss=2.56 avg=2.06\n",
            "[1090 | 3411.05] loss=1.15 avg=2.04\n",
            "[1100 | 3442.19] loss=2.13 avg=2.04\n",
            "[1110 | 3473.34] loss=1.36 avg=2.03\n",
            "[1120 | 3504.48] loss=2.03 avg=2.03\n",
            "[1130 | 3535.64] loss=2.08 avg=2.03\n",
            "[1140 | 3566.74] loss=1.64 avg=2.03\n",
            "[1150 | 3597.94] loss=1.96 avg=2.03\n",
            "[1160 | 3629.18] loss=2.24 avg=2.03\n",
            "[1170 | 3660.36] loss=2.63 avg=2.04\n",
            "[1180 | 3691.53] loss=1.71 avg=2.03\n",
            "[1190 | 3722.67] loss=1.37 avg=2.03\n",
            "[1200 | 3753.78] loss=1.82 avg=2.02\n",
            "[1210 | 3784.90] loss=1.32 avg=2.01\n",
            "[1220 | 3816.05] loss=2.14 avg=2.01\n",
            "[1230 | 3847.22] loss=1.52 avg=2.01\n",
            "[1240 | 3878.33] loss=1.82 avg=2.00\n",
            "[1250 | 3909.48] loss=1.21 avg=1.99\n",
            "[1260 | 3940.70] loss=1.44 avg=1.99\n",
            "[1270 | 3971.89] loss=0.55 avg=1.97\n",
            "[1280 | 4003.12] loss=1.51 avg=1.96\n",
            "[1290 | 4034.36] loss=2.24 avg=1.96\n",
            "[1300 | 4065.52] loss=1.89 avg=1.96\n",
            "[1310 | 4096.82] loss=1.61 avg=1.96\n",
            "[1320 | 4128.05] loss=2.37 avg=1.96\n",
            "[1330 | 4159.30] loss=2.49 avg=1.97\n",
            "[1340 | 4190.58] loss=1.60 avg=1.97\n",
            "[1350 | 4221.83] loss=1.88 avg=1.96\n",
            "[1360 | 4253.09] loss=2.40 avg=1.97\n",
            "[1370 | 4284.32] loss=1.33 avg=1.96\n",
            "[1380 | 4315.56] loss=1.23 avg=1.95\n",
            "[1390 | 4346.75] loss=1.87 avg=1.95\n",
            "[1400 | 4377.99] loss=2.31 avg=1.96\n",
            "[1410 | 4409.24] loss=1.88 avg=1.95\n",
            "[1420 | 4440.46] loss=1.98 avg=1.95\n",
            "[1430 | 4471.60] loss=2.11 avg=1.96\n",
            "[1440 | 4502.85] loss=1.38 avg=1.95\n",
            "[1450 | 4534.07] loss=1.05 avg=1.94\n",
            "[1460 | 4565.18] loss=1.43 avg=1.93\n",
            "[1470 | 4596.28] loss=1.85 avg=1.93\n",
            "[1480 | 4627.43] loss=1.08 avg=1.92\n",
            "[1490 | 4658.58] loss=1.46 avg=1.91\n",
            "[1500 | 4689.71] loss=2.11 avg=1.92\n",
            "[1510 | 4720.91] loss=1.48 avg=1.91\n",
            "[1520 | 4752.09] loss=2.36 avg=1.92\n",
            "[1530 | 4783.32] loss=1.42 avg=1.91\n",
            "[1540 | 4814.55] loss=2.58 avg=1.92\n",
            "[1550 | 4845.79] loss=1.76 avg=1.92\n",
            "[1560 | 4877.04] loss=1.95 avg=1.92\n",
            "[1570 | 4908.28] loss=1.21 avg=1.91\n",
            "[1580 | 4939.55] loss=1.48 avg=1.90\n",
            "[1590 | 4970.77] loss=1.52 avg=1.90\n",
            "[1600 | 5001.98] loss=1.09 avg=1.89\n",
            "[1610 | 5033.22] loss=1.81 avg=1.89\n",
            "[1620 | 5064.44] loss=1.24 avg=1.88\n",
            "[1630 | 5095.66] loss=1.65 avg=1.88\n",
            "[1640 | 5126.92] loss=1.86 avg=1.88\n",
            "[1650 | 5158.18] loss=0.97 avg=1.86\n",
            "[1660 | 5189.38] loss=1.65 avg=1.86\n",
            "[1670 | 5220.53] loss=1.70 avg=1.86\n",
            "[1680 | 5251.58] loss=1.55 avg=1.86\n",
            "[1690 | 5282.62] loss=1.50 avg=1.85\n",
            "[1700 | 5313.80] loss=1.56 avg=1.85\n",
            "[1710 | 5344.97] loss=1.67 avg=1.85\n",
            "[1720 | 5376.02] loss=1.44 avg=1.84\n",
            "[1730 | 5407.13] loss=2.51 avg=1.85\n",
            "[1740 | 5438.27] loss=2.10 avg=1.85\n",
            "[1750 | 5469.35] loss=1.74 avg=1.85\n",
            "[1760 | 5500.42] loss=1.24 avg=1.84\n",
            "[1770 | 5531.45] loss=1.09 avg=1.83\n",
            "[1780 | 5562.50] loss=2.01 avg=1.84\n",
            "[1790 | 5593.54] loss=0.76 avg=1.82\n",
            "[1800 | 5624.57] loss=1.06 avg=1.81\n",
            "[1810 | 5655.55] loss=1.88 avg=1.81\n",
            "[1820 | 5686.64] loss=1.56 avg=1.81\n",
            "[1830 | 5717.70] loss=1.48 avg=1.81\n",
            "[1840 | 5748.75] loss=2.26 avg=1.81\n",
            "[1850 | 5779.84] loss=2.55 avg=1.82\n",
            "[1860 | 5810.91] loss=1.09 avg=1.81\n",
            "[1870 | 5842.01] loss=1.40 avg=1.81\n",
            "[1880 | 5873.04] loss=2.07 avg=1.81\n",
            "[1890 | 5904.10] loss=1.33 avg=1.81\n",
            "[1900 | 5935.20] loss=1.60 avg=1.80\n",
            "[1910 | 5966.24] loss=1.73 avg=1.80\n",
            "[1920 | 5997.32] loss=2.19 avg=1.81\n",
            "[1930 | 6028.47] loss=2.55 avg=1.82\n",
            "[1940 | 6059.58] loss=0.95 avg=1.81\n",
            "[1950 | 6090.70] loss=2.35 avg=1.81\n",
            "[1960 | 6121.80] loss=2.10 avg=1.82\n",
            "[1970 | 6152.99] loss=1.54 avg=1.81\n",
            "[1980 | 6184.09] loss=1.59 avg=1.81\n",
            "[1990 | 6215.22] loss=2.32 avg=1.82\n",
            "[2000 | 6246.36] loss=1.63 avg=1.81\n",
            "Saving checkpoint/run42K/model-2000\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/training/saver.py:963: remove_checkpoint (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use standard file APIs to delete files with this prefix.\n",
            "[2010 | 6285.78] loss=1.37 avg=1.81\n",
            "[2020 | 6316.82] loss=2.57 avg=1.82\n",
            "[2030 | 6347.86] loss=0.60 avg=1.80\n",
            "[2040 | 6378.93] loss=1.15 avg=1.80\n",
            "[2050 | 6409.96] loss=1.62 avg=1.79\n",
            "[2060 | 6440.98] loss=2.71 avg=1.80\n",
            "[2070 | 6472.05] loss=0.53 avg=1.79\n",
            "[2080 | 6503.08] loss=1.84 avg=1.79\n",
            "[2090 | 6534.18] loss=2.46 avg=1.80\n",
            "[2100 | 6565.24] loss=2.37 avg=1.80\n",
            "[2110 | 6596.38] loss=0.98 avg=1.79\n",
            "[2120 | 6627.50] loss=1.21 avg=1.79\n",
            "[2130 | 6658.60] loss=1.09 avg=1.78\n",
            "[2140 | 6689.66] loss=1.39 avg=1.78\n",
            "[2150 | 6720.76] loss=1.16 avg=1.77\n",
            "[2160 | 6751.85] loss=1.37 avg=1.76\n",
            "[2170 | 6782.88] loss=1.10 avg=1.76\n",
            "[2180 | 6813.86] loss=1.88 avg=1.76\n",
            "[2190 | 6844.91] loss=1.75 avg=1.76\n",
            "[2200 | 6875.94] loss=1.07 avg=1.75\n",
            "[2210 | 6906.94] loss=2.32 avg=1.76\n",
            "[2220 | 6937.97] loss=1.35 avg=1.75\n",
            "[2230 | 6969.05] loss=1.89 avg=1.75\n",
            "[2240 | 7000.13] loss=0.32 avg=1.74\n",
            "[2250 | 7031.22] loss=1.17 avg=1.73\n",
            "[2260 | 7062.33] loss=1.63 avg=1.73\n",
            "[2270 | 7093.41] loss=1.55 avg=1.73\n",
            "[2280 | 7124.44] loss=0.61 avg=1.72\n",
            "[2290 | 7155.57] loss=1.36 avg=1.71\n",
            "[2300 | 7186.70] loss=1.84 avg=1.71\n",
            "[2310 | 7217.79] loss=1.57 avg=1.71\n",
            "[2320 | 7248.92] loss=1.16 avg=1.71\n",
            "[2330 | 7279.94] loss=1.10 avg=1.70\n",
            "[2340 | 7311.06] loss=1.63 avg=1.70\n",
            "[2350 | 7342.17] loss=0.87 avg=1.69\n",
            "[2360 | 7373.22] loss=0.60 avg=1.68\n",
            "[2370 | 7404.34] loss=1.69 avg=1.68\n",
            "[2380 | 7435.41] loss=0.41 avg=1.66\n",
            "[2390 | 7466.46] loss=1.90 avg=1.67\n",
            "[2400 | 7497.61] loss=1.19 avg=1.66\n",
            "[2410 | 7528.67] loss=1.58 avg=1.66\n",
            "[2420 | 7559.66] loss=0.58 avg=1.65\n",
            "[2430 | 7590.75] loss=1.64 avg=1.65\n",
            "[2440 | 7621.85] loss=1.78 avg=1.65\n",
            "[2450 | 7652.90] loss=1.12 avg=1.64\n",
            "[2460 | 7683.99] loss=1.11 avg=1.64\n",
            "[2470 | 7715.06] loss=0.58 avg=1.63\n",
            "[2480 | 7746.10] loss=1.92 avg=1.63\n",
            "[2490 | 7777.19] loss=0.87 avg=1.62\n",
            "[2500 | 7808.22] loss=2.56 avg=1.63\n",
            "[2510 | 7839.32] loss=1.95 avg=1.63\n",
            "[2520 | 7870.38] loss=1.42 avg=1.63\n",
            "[2530 | 7901.46] loss=0.80 avg=1.62\n",
            "[2540 | 7932.51] loss=0.93 avg=1.62\n",
            "[2550 | 7963.58] loss=0.52 avg=1.60\n",
            "[2560 | 7994.64] loss=0.60 avg=1.59\n",
            "[2570 | 8025.70] loss=0.83 avg=1.58\n",
            "[2580 | 8056.75] loss=0.47 avg=1.57\n",
            "[2590 | 8087.83] loss=0.89 avg=1.57\n",
            "[2600 | 8118.87] loss=1.72 avg=1.57\n",
            "[2610 | 8149.95] loss=0.66 avg=1.56\n",
            "[2620 | 8181.01] loss=0.87 avg=1.55\n",
            "[2630 | 8212.05] loss=0.97 avg=1.54\n",
            "[2640 | 8243.11] loss=1.24 avg=1.54\n",
            "[2650 | 8274.14] loss=0.61 avg=1.53\n",
            "[2660 | 8305.22] loss=0.66 avg=1.52\n",
            "[2670 | 8336.30] loss=0.83 avg=1.51\n",
            "[2680 | 8367.36] loss=1.10 avg=1.51\n",
            "[2690 | 8398.50] loss=1.76 avg=1.51\n",
            "[2700 | 8429.66] loss=1.84 avg=1.52\n",
            "[2710 | 8460.80] loss=1.46 avg=1.51\n",
            "[2720 | 8492.00] loss=1.29 avg=1.51\n",
            "[2730 | 8523.14] loss=1.20 avg=1.51\n",
            "[2740 | 8554.29] loss=0.74 avg=1.50\n",
            "[2750 | 8585.34] loss=1.39 avg=1.50\n",
            "[2760 | 8616.43] loss=1.07 avg=1.49\n",
            "[2770 | 8647.60] loss=1.05 avg=1.49\n",
            "[2780 | 8678.71] loss=1.03 avg=1.49\n",
            "[2790 | 8709.85] loss=1.00 avg=1.48\n",
            "[2800 | 8740.96] loss=1.71 avg=1.48\n",
            "[2810 | 8772.10] loss=0.74 avg=1.47\n",
            "[2820 | 8803.20] loss=0.44 avg=1.46\n",
            "[2830 | 8834.30] loss=1.31 avg=1.46\n",
            "[2840 | 8865.43] loss=0.75 avg=1.45\n",
            "[2850 | 8896.54] loss=1.58 avg=1.46\n",
            "[2860 | 8927.60] loss=2.02 avg=1.46\n",
            "[2870 | 8958.65] loss=0.62 avg=1.45\n",
            "[2880 | 8989.71] loss=0.70 avg=1.44\n",
            "[2890 | 9020.82] loss=1.73 avg=1.45\n",
            "[2900 | 9051.95] loss=0.45 avg=1.44\n",
            "[2910 | 9083.02] loss=0.90 avg=1.43\n",
            "[2920 | 9114.03] loss=2.12 avg=1.44\n",
            "[2930 | 9145.04] loss=0.64 avg=1.43\n",
            "[2940 | 9176.08] loss=1.50 avg=1.43\n",
            "[2950 | 9207.18] loss=1.52 avg=1.43\n",
            "[2960 | 9238.24] loss=2.35 avg=1.44\n",
            "[2970 | 9269.30] loss=0.78 avg=1.44\n",
            "[2980 | 9300.51] loss=0.62 avg=1.43\n",
            "[2990 | 9331.67] loss=1.50 avg=1.43\n",
            "[3000 | 9362.83] loss=0.17 avg=1.41\n",
            "Saving checkpoint/run42K/model-3000\n",
            "[3010 | 9401.43] loss=0.66 avg=1.41\n",
            "[3020 | 9432.49] loss=0.57 avg=1.40\n",
            "[3030 | 9463.59] loss=1.29 avg=1.40\n",
            "[3040 | 9494.64] loss=1.45 avg=1.40\n",
            "[3050 | 9525.71] loss=1.20 avg=1.39\n",
            "[3060 | 9556.80] loss=0.98 avg=1.39\n",
            "[3070 | 9587.90] loss=0.31 avg=1.38\n",
            "[3080 | 9619.00] loss=0.54 avg=1.37\n",
            "[3090 | 9650.10] loss=0.54 avg=1.36\n",
            "[3100 | 9681.16] loss=1.51 avg=1.36\n",
            "[3110 | 9712.20] loss=0.98 avg=1.36\n",
            "[3120 | 9743.25] loss=0.40 avg=1.35\n",
            "[3130 | 9774.29] loss=1.14 avg=1.35\n",
            "[3140 | 9805.33] loss=0.68 avg=1.34\n",
            "[3150 | 9836.39] loss=0.63 avg=1.33\n",
            "[3160 | 9867.45] loss=0.56 avg=1.32\n",
            "[3170 | 9898.42] loss=0.92 avg=1.32\n",
            "[3180 | 9929.51] loss=2.22 avg=1.33\n",
            "[3190 | 9960.59] loss=0.41 avg=1.32\n",
            "[3200 | 9991.68] loss=1.64 avg=1.32\n",
            "[3210 | 10022.67] loss=0.40 avg=1.31\n",
            "[3220 | 10053.74] loss=1.09 avg=1.31\n",
            "[3230 | 10084.88] loss=0.53 avg=1.30\n",
            "[3240 | 10115.95] loss=1.35 avg=1.30\n",
            "[3250 | 10147.09] loss=0.44 avg=1.29\n",
            "[3260 | 10178.27] loss=0.95 avg=1.29\n",
            "[3270 | 10209.37] loss=0.53 avg=1.28\n",
            "[3280 | 10240.56] loss=0.80 avg=1.28\n",
            "[3290 | 10271.65] loss=1.02 avg=1.28\n",
            "[3300 | 10302.70] loss=0.61 avg=1.27\n",
            "[3310 | 10333.81] loss=0.63 avg=1.26\n",
            "[3320 | 10365.00] loss=1.25 avg=1.26\n",
            "[3330 | 10396.17] loss=0.64 avg=1.26\n",
            "[3340 | 10427.34] loss=0.88 avg=1.25\n",
            "[3350 | 10458.55] loss=0.52 avg=1.24\n",
            "[3360 | 10489.83] loss=1.04 avg=1.24\n",
            "[3370 | 10521.11] loss=0.52 avg=1.23\n",
            "[3380 | 10552.41] loss=0.37 avg=1.23\n",
            "[3390 | 10583.73] loss=0.85 avg=1.22\n",
            "[3400 | 10615.00] loss=0.94 avg=1.22\n",
            "[3410 | 10646.28] loss=0.54 avg=1.21\n",
            "[3420 | 10677.54] loss=0.98 avg=1.21\n",
            "[3430 | 10708.86] loss=0.61 avg=1.20\n",
            "[3440 | 10740.14] loss=0.84 avg=1.20\n",
            "[3450 | 10771.45] loss=0.43 avg=1.19\n",
            "[3460 | 10802.78] loss=1.86 avg=1.20\n",
            "[3470 | 10834.11] loss=0.49 avg=1.19\n",
            "[3480 | 10865.37] loss=1.49 avg=1.19\n",
            "[3490 | 10896.59] loss=0.58 avg=1.19\n",
            "[3500 | 10927.86] loss=1.37 avg=1.19\n",
            "[3510 | 10959.12] loss=0.74 avg=1.19\n",
            "[3520 | 10990.32] loss=0.96 avg=1.18\n",
            "[3530 | 11021.53] loss=1.09 avg=1.18\n",
            "[3540 | 11052.71] loss=0.95 avg=1.18\n",
            "[3550 | 11083.97] loss=0.90 avg=1.18\n",
            "[3560 | 11115.24] loss=1.12 avg=1.18\n",
            "[3570 | 11146.52] loss=0.81 avg=1.17\n",
            "[3580 | 11177.74] loss=0.83 avg=1.17\n",
            "[3590 | 11208.98] loss=1.85 avg=1.18\n",
            "[3600 | 11240.27] loss=0.57 avg=1.17\n",
            "[3610 | 11271.56] loss=0.35 avg=1.16\n",
            "[3620 | 11302.82] loss=0.46 avg=1.15\n",
            "[3630 | 11334.10] loss=0.42 avg=1.15\n",
            "[3640 | 11365.32] loss=0.47 avg=1.14\n",
            "[3650 | 11396.52] loss=0.88 avg=1.14\n",
            "[3660 | 11427.70] loss=0.58 avg=1.13\n",
            "[3670 | 11458.89] loss=0.35 avg=1.12\n",
            "[3680 | 11490.09] loss=0.52 avg=1.12\n",
            "[3690 | 11521.29] loss=1.62 avg=1.12\n",
            "[3700 | 11552.45] loss=0.36 avg=1.11\n",
            "[3710 | 11583.65] loss=0.49 avg=1.11\n",
            "[3720 | 11614.89] loss=0.23 avg=1.10\n",
            "[3730 | 11646.09] loss=0.78 avg=1.10\n",
            "[3740 | 11677.33] loss=0.75 avg=1.09\n",
            "[3750 | 11708.48] loss=0.39 avg=1.08\n",
            "[3760 | 11739.69] loss=1.28 avg=1.09\n",
            "[3770 | 11770.89] loss=1.28 avg=1.09\n",
            "[3780 | 11802.05] loss=0.10 avg=1.08\n",
            "[3790 | 11833.23] loss=0.63 avg=1.07\n",
            "[3800 | 11864.44] loss=0.52 avg=1.07\n",
            "[3810 | 11895.63] loss=0.51 avg=1.06\n",
            "[3820 | 11926.88] loss=0.97 avg=1.06\n",
            "[3830 | 11958.09] loss=0.65 avg=1.06\n",
            "[3840 | 11989.33] loss=0.90 avg=1.06\n",
            "[3850 | 12020.59] loss=0.48 avg=1.05\n",
            "[3860 | 12051.91] loss=0.19 avg=1.04\n",
            "[3870 | 12083.18] loss=0.49 avg=1.04\n",
            "[3880 | 12114.47] loss=0.27 avg=1.03\n",
            "[3890 | 12145.74] loss=1.27 avg=1.03\n",
            "[3900 | 12177.01] loss=0.29 avg=1.02\n",
            "[3910 | 12208.26] loss=0.77 avg=1.02\n",
            "[3920 | 12239.55] loss=0.89 avg=1.02\n",
            "[3930 | 12270.79] loss=0.51 avg=1.01\n",
            "[3940 | 12302.03] loss=0.36 avg=1.01\n",
            "[3950 | 12333.31] loss=1.06 avg=1.01\n",
            "[3960 | 12364.55] loss=1.50 avg=1.01\n",
            "[3970 | 12395.74] loss=0.34 avg=1.01\n",
            "[3980 | 12427.06] loss=1.09 avg=1.01\n",
            "[3990 | 12458.30] loss=0.32 avg=1.00\n",
            "[4000 | 12489.64] loss=0.48 avg=0.99\n",
            "Saving checkpoint/run42K/model-4000\n",
            "[4010 | 12528.52] loss=1.42 avg=1.00\n",
            "[4020 | 12559.87] loss=0.91 avg=1.00\n",
            "[4030 | 12591.22] loss=0.23 avg=0.99\n",
            "[4040 | 12622.48] loss=0.12 avg=0.98\n",
            "[4050 | 12653.65] loss=0.37 avg=0.97\n",
            "[4060 | 12684.93] loss=0.24 avg=0.97\n",
            "[4070 | 12716.20] loss=1.08 avg=0.97\n",
            "[4080 | 12747.53] loss=0.45 avg=0.96\n",
            "[4090 | 12778.87] loss=1.15 avg=0.96\n",
            "[4100 | 12810.16] loss=1.20 avg=0.97\n",
            "[4110 | 12841.49] loss=1.68 avg=0.97\n",
            "[4120 | 12872.76] loss=1.06 avg=0.98\n",
            "[4130 | 12904.15] loss=0.98 avg=0.98\n",
            "[4140 | 12935.45] loss=0.64 avg=0.97\n",
            "[4150 | 12966.65] loss=0.37 avg=0.97\n",
            "[4160 | 12997.75] loss=0.64 avg=0.96\n",
            "[4170 | 13028.91] loss=0.57 avg=0.96\n",
            "[4180 | 13060.19] loss=0.39 avg=0.95\n",
            "[4190 | 13091.38] loss=0.16 avg=0.94\n",
            "[4200 | 13122.60] loss=0.64 avg=0.94\n",
            "[4210 | 13153.78] loss=0.54 avg=0.94\n",
            "[4220 | 13185.02] loss=0.31 avg=0.93\n",
            "[4230 | 13216.27] loss=1.87 avg=0.94\n",
            "[4240 | 13247.51] loss=0.48 avg=0.94\n",
            "[4250 | 13278.77] loss=0.78 avg=0.93\n",
            "[4260 | 13309.98] loss=1.40 avg=0.94\n",
            "[4270 | 13341.12] loss=0.19 avg=0.93\n",
            "[4280 | 13372.31] loss=0.76 avg=0.93\n",
            "[4290 | 13403.61] loss=0.32 avg=0.92\n",
            "[4300 | 13434.90] loss=0.18 avg=0.92\n",
            "[4310 | 13466.14] loss=0.78 avg=0.92\n",
            "[4320 | 13497.40] loss=0.32 avg=0.91\n",
            "[4330 | 13528.73] loss=0.39 avg=0.90\n",
            "[4340 | 13560.01] loss=0.16 avg=0.90\n",
            "[4350 | 13591.29] loss=0.67 avg=0.89\n",
            "[4360 | 13622.58] loss=0.60 avg=0.89\n",
            "[4370 | 13653.88] loss=0.36 avg=0.89\n",
            "[4380 | 13685.17] loss=0.38 avg=0.88\n",
            "[4390 | 13716.48] loss=0.30 avg=0.87\n",
            "[4400 | 13747.81] loss=0.23 avg=0.87\n",
            "[4410 | 13779.05] loss=0.37 avg=0.86\n",
            "[4420 | 13810.35] loss=0.93 avg=0.86\n",
            "[4430 | 13841.58] loss=0.96 avg=0.86\n",
            "[4440 | 13872.90] loss=0.26 avg=0.86\n",
            "[4450 | 13904.18] loss=0.39 avg=0.85\n",
            "[4460 | 13935.47] loss=0.81 avg=0.85\n",
            "[4470 | 13966.72] loss=0.10 avg=0.85\n",
            "[4480 | 13998.02] loss=0.26 avg=0.84\n",
            "[4490 | 14029.27] loss=1.98 avg=0.85\n",
            "[4500 | 14060.56] loss=0.68 avg=0.85\n",
            "[4510 | 14091.85] loss=0.19 avg=0.84\n",
            "[4520 | 14123.13] loss=0.21 avg=0.84\n",
            "[4530 | 14154.36] loss=0.30 avg=0.83\n",
            "[4540 | 14185.65] loss=0.53 avg=0.83\n",
            "[4550 | 14216.87] loss=0.40 avg=0.82\n",
            "[4560 | 14248.22] loss=0.93 avg=0.82\n",
            "[4570 | 14279.48] loss=0.14 avg=0.82\n",
            "[4580 | 14310.76] loss=0.53 avg=0.82\n",
            "[4590 | 14342.03] loss=0.18 avg=0.81\n",
            "[4600 | 14373.31] loss=0.43 avg=0.80\n",
            "[4610 | 14404.61] loss=0.93 avg=0.81\n",
            "[4620 | 14435.83] loss=0.11 avg=0.80\n",
            "[4630 | 14467.12] loss=0.89 avg=0.80\n",
            "[4640 | 14498.41] loss=0.78 avg=0.80\n",
            "[4650 | 14529.73] loss=0.35 avg=0.80\n",
            "[4660 | 14560.99] loss=0.62 avg=0.79\n",
            "[4670 | 14592.33] loss=1.37 avg=0.80\n",
            "[4680 | 14623.62] loss=0.37 avg=0.79\n",
            "[4690 | 14654.89] loss=0.17 avg=0.79\n",
            "[4700 | 14686.22] loss=1.39 avg=0.79\n",
            "[4710 | 14717.49] loss=1.12 avg=0.80\n",
            "[4720 | 14748.79] loss=0.19 avg=0.79\n",
            "[4730 | 14780.10] loss=0.13 avg=0.79\n",
            "[4740 | 14811.34] loss=0.24 avg=0.78\n",
            "[4750 | 14842.68] loss=0.15 avg=0.77\n",
            "[4760 | 14874.10] loss=0.16 avg=0.77\n",
            "[4770 | 14905.42] loss=0.24 avg=0.76\n",
            "[4780 | 14936.78] loss=0.42 avg=0.76\n",
            "[4790 | 14968.18] loss=0.49 avg=0.76\n",
            "[4800 | 14999.57] loss=0.39 avg=0.75\n",
            "[4810 | 15030.96] loss=0.16 avg=0.75\n",
            "[4820 | 15062.29] loss=0.15 avg=0.74\n",
            "[4830 | 15093.64] loss=0.32 avg=0.74\n",
            "[4840 | 15124.96] loss=1.84 avg=0.75\n",
            "[4850 | 15156.34] loss=0.34 avg=0.74\n",
            "[4860 | 15187.61] loss=0.56 avg=0.74\n",
            "[4870 | 15218.91] loss=0.16 avg=0.74\n",
            "[4880 | 15250.26] loss=0.12 avg=0.73\n",
            "[4890 | 15281.61] loss=0.80 avg=0.73\n",
            "[4900 | 15312.92] loss=0.14 avg=0.72\n",
            "[4910 | 15344.25] loss=0.68 avg=0.72\n",
            "[4920 | 15375.61] loss=1.98 avg=0.74\n",
            "[4930 | 15406.93] loss=0.13 avg=0.73\n",
            "[4940 | 15438.20] loss=0.57 avg=0.73\n",
            "[4950 | 15469.51] loss=0.47 avg=0.73\n",
            "[4960 | 15500.81] loss=0.64 avg=0.72\n",
            "[4970 | 15532.13] loss=0.58 avg=0.72\n",
            "[4980 | 15563.46] loss=0.19 avg=0.72\n",
            "[4990 | 15594.77] loss=0.35 avg=0.71\n",
            "[5000 | 15626.05] loss=0.27 avg=0.71\n",
            "Saving checkpoint/run42K/model-5000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VHdTL8NDbAh3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "gpt2.copy_checkpoint_to_gdrive(run_name='run42K')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gApI-bHb6TsH",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pel-uBULXO2L",
        "colab_type": "text"
      },
      "source": [
        "## Load a Trained Model Checkpoint"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DCcx5u7sbPTD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "gpt2.copy_checkpoint_from_gdrive(run_name='run42K')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-fxL77nvAMAX",
        "colab_type": "code",
        "outputId": "6f1517b4-77ae-433e-e8a8-2bf6ec6c4ece",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "sess = gpt2.start_tf_sess()\n",
        "gpt2.load_gpt2(sess, run_name='run42K')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loading checkpoint checkpoint/run42K/model-5000\n",
            "INFO:tensorflow:Restoring parameters from checkpoint/run42K/model-5000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ClJwpF_ACONp",
        "colab_type": "text"
      },
      "source": [
        "## Generate Text From The Trained Model\n",
        "\n",
        "Same as normal generate functions, except with additional parameters to handle the new tokens."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4RNY6RBI9LmL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "gpt2.generate(sess, run_name='run42K',\n",
        "             length=100,\n",
        "             prefix=\"<|startoftext|>\",\n",
        "             truncate=\"<|endoftext|>\",\n",
        "             include_prefix=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8DKMc0fiej4N",
        "colab_type": "code",
        "outputId": "6a4e25da-2d68-44eb-d500-c71686336419",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 377
        }
      },
      "source": [
        "gpt2.generate(sess, run_name='run42K',\n",
        "              length=100,\n",
        "              temperature=.7,\n",
        "              nsamples=10,\n",
        "              batch_size=10,\n",
        "              prefix=\"[WP] You discover an alternate reality\",\n",
        "              truncate=\"<|endoftext|>\",\n",
        "              include_prefix=True\n",
        "              )"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[WP] You discover an alternate reality where you are the \"chosen one\" and the council of gods keeps changing the world to fit your fancy. Unfortunately, the competition is pretty boring so you settle on this one...fake?\n",
            "====================\n",
            "[WP] You discover an alternate reality where you are rich and famous for your charitable giving, and people are genuinely touched by your generosity. A genuine sense of joy and satisfaction builds up in your heart. This is your first \"life\".\n",
            "====================\n",
            "[WP] You discover an alternate reality where humans and other animals live together in harmony. You're all set to become extinct but the species that first separated humans from the rest of the world manages to persist and gain a foothold in another part of the universe. This is your life now.\"\n",
            "====================\n",
            "[WP] You discover an alternate reality where people can bank sleep and never wake up to it.com is a very real and very convenient method for getting things done in the real world.\n",
            "====================\n",
            "[WP] You discover an alternate reality where humans ruled the galaxy until the dawn of man. After thousands of years, aliens invade Earth, but could never catch a break; the humans had mastered every aspect of the galaxy and now, aliens invade us again.\n",
            "====================\n",
            "[WP] You discover an alternate reality where everyone gets a dollar for every truth they say. The dollar is actually a Ponzi scheme, but you and millions of other people believe it's an honest-to-God scheme... So don't trust any of the people on the internet.\n",
            "====================\n",
            "[WP] You discover an alternate reality where everyone on Earth is able to fly. However, you and everyone else are limited by your ability to the only thing that can't: die. The thing is, you're not able to kill people either.\n",
            "====================\n",
            "[WP] You discover an alternate reality where people can fight to the death, the battle between good and evil has been settled for ages. One day, you witness a man versus woman fight to the death, the outcome is determined solely by the spectators. You witness the first ever battle between a good and evil woman.\n",
            "====================\n",
            "[WP] You discover an alternate reality in which people have magic and ultra-magical abilities. You find out you're not very good at anything compared to the rest of the world, but that's okay, because your pseudo-magic is so good that it makes everything else look bad.\n",
            "====================\n",
            "[WP] You discover an alternate reality where people can fly, the problem is that the wind blows harder and harder against you every step you take.\n",
            "====================\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vnpv461FRfBL",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mP7iHAhCAEKo",
        "colab_type": "text"
      },
      "source": [
        "If generating in bulk, you may want to set `sample_demin=''` to remove the delimiter between each sample."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fa6p6arifSL0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "gen_file = 'gpt2_gentext_{:%Y%m%d_%H%M%S}.txt'.format(datetime.utcnow())\n",
        "\n",
        "gpt2.generate_to_file(sess,\n",
        "                      destination_path=gen_file,\n",
        "                      length=100,\n",
        "                      temperature=1.0,\n",
        "                      nsamples=100,\n",
        "                      batch_size=20,\n",
        "                      prefix=\"<|startoftext|>\",\n",
        "                      truncate=\"<|endoftext|>\",\n",
        "                      include_prefix=False,\n",
        "                      sample_delim=''\n",
        "                      )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0-LRex8lfv1g",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# may have to run twice to get file to download\n",
        "files.download(gen_file)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ig-KVgkCDCKD",
        "colab_type": "text"
      },
      "source": [
        "# Etcetera\n",
        "\n",
        "If the notebook has errors (e.g. GPU Sync Fail), force-kill the Colaboratory virtual machine and restart it with the command below:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rIHiVP53FnsX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!kill -9 -1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wmTXWNUygS5E",
        "colab_type": "text"
      },
      "source": [
        "# LICENSE\n",
        "\n",
        "MIT License\n",
        "\n",
        "Copyright (c) 2019 Max Woolf\n",
        "\n",
        "Permission is hereby granted, free of charge, to any person obtaining a copy\n",
        "of this software and associated documentation files (the \"Software\"), to deal\n",
        "in the Software without restriction, including without limitation the rights\n",
        "to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n",
        "copies of the Software, and to permit persons to whom the Software is\n",
        "furnished to do so, subject to the following conditions:\n",
        "\n",
        "The above copyright notice and this permission notice shall be included in all\n",
        "copies or substantial portions of the Software.\n",
        "\n",
        "THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n",
        "IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n",
        "FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n",
        "AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n",
        "LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n",
        "OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n",
        "SOFTWARE."
      ]
    }
  ]
}